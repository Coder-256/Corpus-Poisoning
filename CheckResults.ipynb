{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GloveRun class to load and interpret data\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from math import exp, inf, log, sqrt\n",
    "from functools import lru_cache\n",
    "\n",
    "# Load embeddings: these are trained with the full \"Wikipedia\" configuration\n",
    "cooccur1_path = \"../GloVe/wikipedia/paper/cooccurrence.bin\"\n",
    "cooccur2_path = \"../GloVe/wikipedia/paper/cooccurrence2.bin\"\n",
    "cooccur1_filt_path = \"../GloVe/wikipedia/paper/cooccurrence.filt.bin\"\n",
    "cooccur2_filt_path = \"../GloVe/wikipedia/paper/cooccurrence2.filt.bin\"\n",
    "vocab1_path = \"../GloVe/wikipedia/paper/vocab.txt\"\n",
    "vocab2_path = \"../GloVe/wikipedia/paper/vocab2.txt\"\n",
    "vector1_path = \"../GloVe/wikipedia/paper/vectors.bin\"\n",
    "vector2_path = \"../GloVe/wikipedia/paper/vectors2.bin\"\n",
    "vector1_size = 100\n",
    "vector2_size = 100\n",
    "\n",
    "\n",
    "class GloveRun:\n",
    "  B: np.ndarray      # bias vector\n",
    "  C: csr_matrix      # cooccurrence matrix\n",
    "  Csum: np.ndarray   # sum of cooccurrence rows\n",
    "  dictionary: list   # dictionary list (of strings)\n",
    "  freq: list         # frequency list\n",
    "  word: np.ndarray   # matrix of word vectors\n",
    "  ctx: np.ndarray    # matrix of context vectors\n",
    "  evec: np.ndarray   # matrix of \"e\" vectors (word+context)\n",
    "  enorm: np.ndarray  # vector of norms of each \"e\" vector\n",
    "  pairs: list        # list of word pairs\n",
    "  df: pd.DataFrame   # pandas dataframe with results\n",
    "\n",
    "  def __init__(self, name, cooccur_path, cooccur_filt_path, vocab_path, vector_path, vector_size):\n",
    "    self.name = name\n",
    "    self.cooccur_path = cooccur_path\n",
    "    self.cooccur_filt_path = cooccur_filt_path\n",
    "    self.vocab_path = vocab_path\n",
    "    self.vector_path = vector_path\n",
    "    self.vector_size = vector_size  # AKA \"embedding dimension\"\n",
    "\n",
    "    self.B = None\n",
    "    self.C = None\n",
    "    self.Csum = None\n",
    "    self.dictionary = None\n",
    "    self.freq = None\n",
    "    self.word = None\n",
    "    self.ctx = None\n",
    "    self.evec = None\n",
    "    self.enorm = None\n",
    "    self.pairs = None\n",
    "\n",
    "  # Loading functions:\n",
    "\n",
    "  def load_vocab(self):\n",
    "    self.dictionary = []\n",
    "    self.freq = []\n",
    "    with open(self.vocab_path, \"r\") as f:\n",
    "      for line in f:\n",
    "        word, freq = line.split(' ')\n",
    "        self.dictionary.append(word)\n",
    "        self.freq.append(freq)\n",
    "    self.vocab_size = len(self.dictionary)\n",
    "    self.word_idx = {w: i for i, w in enumerate(self.dictionary)}\n",
    "\n",
    "  def load_vectors(self):\n",
    "    dt = np.dtype('<f8')\n",
    "    arr = np.fromfile(self.vector_path, dtype=dt)\n",
    "    vecs = arr.reshape((2*self.vocab_size, self.vector_size+1))\n",
    "    word_mat, ctx_mat = np.split(vecs, 2)\n",
    "    self.word = word_mat[:, :self.vector_size]\n",
    "    self.ctx = ctx_mat[:, :self.vector_size]\n",
    "    self.B = word_mat[:, self.vector_size]\n",
    "    self.evec = self.word + self.ctx\n",
    "    self.enorm = np.linalg.norm(self.evec, axis=1)\n",
    "    # note: bias_ctx (aka b'), stored in ctx_mat[:, vector_size], is unused\n",
    "    # (see the footnote on p. 5)\n",
    "\n",
    "\n",
    "  def load_cooccurrences(self):\n",
    "    if not os.path.isfile(self.cooccur_filt_path):\n",
    "      # run filt-cooccur.sh to filter the cooccurrence matrix to only\n",
    "      # the indices we are interested in: the indices of our word pairs\n",
    "      indices = {i for p in self.pairs for i in p}\n",
    "\n",
    "      with open(self.cooccur_filt_path + \".tmp\", \"wb\") as f:\n",
    "        print(\"calling filter subproces...\")\n",
    "        args = [\"./filt-cooccur.sh\", self.cooccur_path] + \\\n",
    "            [str(i) for i in indices]\n",
    "        subprocess.run(args, stdout=f, check=True)\n",
    "        print(\"subprocess exited normally.\")\n",
    "\n",
    "      os.rename(self.cooccur_filt_path + \".tmp\", self.cooccur_filt_path)\n",
    "\n",
    "    # read the filtered cooccurrence matrix\n",
    "    # entries are stored in COO format (see CREC from GloVe/src/common.h)\n",
    "    # we convert to CSR format for faster access\n",
    "    dt = np.dtype([('i', '<i4'), ('j', '<i4'), ('x', '<f8')])\n",
    "    arr = np.fromfile(self.cooccur_filt_path, dtype=dt)\n",
    "    self.C = csr_matrix((arr['x'], (arr['i']-1, arr['j']-1)))\n",
    "    self.Csum = self.C.sum(axis=1).A1\n",
    "\n",
    "  def setup_pairs(self, str_pairs):\n",
    "    self.pairs = [(self.word_idx[s], self.word_idx[t]) for s, t in str_pairs]\n",
    "\n",
    "  # Helper functions:\n",
    "\n",
    "  def model_f(self, u: int, v: int, c: float, epsilon: float) -> float:\n",
    "    \"\"\" Optimized f() \"\"\"\n",
    "    logc = log(c) if c > 0 else -inf\n",
    "    return max(logc-self.B[u]-self.B[v], epsilon)\n",
    "\n",
    "  def M_row(self, u):\n",
    "    \"\"\" Calculate a sparse row of M \"\"\"\n",
    "    C_row = self.C.getrow(u)\n",
    "    cols = C_row.nonzero()[1]\n",
    "    vals = np.fromiter((self.model_f(u, v, self.C[u, v], 0)\n",
    "                        for v in cols), dtype=np.float64)\n",
    "    return csr_matrix((vals, C_row.indices, C_row.indptr), C_row.shape)\n",
    "  \n",
    "  def get_coss(self, t):\n",
    "    \"\"\" Compute the cosine similarity for each word relative to the target \"\"\"\n",
    "    dots = self.evec @ self.evec[t].transpose()\n",
    "    return dots / (self.enorm[t]*self.enorm)\n",
    "\n",
    "  # Similarity functions:\n",
    "\n",
    "  def cos_sim(self, s, t):\n",
    "    \"\"\" Calculate the embedding similarity between the two given words \"\"\"\n",
    "    es = self.evec[s]\n",
    "    et = self.evec[t]\n",
    "    return es.dot(et)/(np.linalg.norm(es)*np.linalg.norm(et))\n",
    "\n",
    "  def sim1(self, s, t):\n",
    "    \"\"\" Calculate the distributional sim1 \"\"\"\n",
    "    num = self.model_f(s, t, self.C[s, t], 0)\n",
    "    den1 = self.model_f(s, t, self.Csum[s], exp(-60))\n",
    "    den2 = self.model_f(s, t, self.Csum[t], exp(-60))\n",
    "    return num/sqrt(den1*den2)\n",
    "\n",
    "  @lru_cache(None) # worst-case runtime is O(vocab_size), worth memoizing\n",
    "  def sim2(self, s, t):\n",
    "    \"\"\" Calculate the distributional sim2 \"\"\"\n",
    "    Ms = self.M_row(s)\n",
    "    Mt = self.M_row(t)\n",
    "    dot = (Ms @ Mt.transpose())[0, 0]\n",
    "    Ms_norm2 = Ms.power(2).sum()\n",
    "    Mt_norm2 = Mt.power(2).sum()\n",
    "    return dot/sqrt(Ms_norm2*Mt_norm2)\n",
    "\n",
    "  def sim12(self, s, t):\n",
    "    \"\"\" Calculate the distributional sim1+2 \"\"\"\n",
    "    return (self.sim1(s, t)+self.sim2(s, t))/2\n",
    "\n",
    "  def rank(self, s, t):\n",
    "    \"\"\" Calculate the rank of the given source word relative to the target word \"\"\"\n",
    "    coss = self.get_coss(t)\n",
    "    # find number of words with a greater cos similarity than `s` to the target\n",
    "    # (including `t` itself, so rank 1 is the closest word to `t` except `t` itself)\n",
    "    return np.sum(coss > coss[s])\n",
    "\n",
    "  # Rank lists:\n",
    "\n",
    "  def top_ranked(self, t, n):\n",
    "    \"\"\" Find the top n highest-ranked words relative to the given target \"\"\"\n",
    "    coss = self.get_coss(t)\n",
    "    # find the indices of the n+1 words w/ the highest cos similarity\n",
    "    res = np.argpartition(coss, -(n+1))[-(n+1):]\n",
    "    # sort the indices by cosine similarity\n",
    "    res = res[np.argsort(coss[res])]\n",
    "    # return indices 0 through n-2 in descending order (index n-1 is `t`)\n",
    "    return res[-2::-1]\n",
    "\n",
    "  def top_ranked_str(self, t, n):\n",
    "    \"\"\" Get the top-ranked strings for the given target \"\"\"\n",
    "    return [self.dictionary[i] for i in self.top_ranked(t, n)]\n",
    "\n",
    "\n",
    "# GloVe-no attack\n",
    "run1 = GloveRun(\"orig \", cooccur1_path, cooccur1_filt_path,\n",
    "                vocab1_path, vector1_path, vector1_size)\n",
    "# GloVe-paper\n",
    "run2 = GloveRun(\"paper\", cooccur2_path, cooccur2_filt_path,\n",
    "                vocab2_path, vector2_path, vector2_size)\n",
    "runs = [run1, run2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the input data\n",
    "\n",
    "print(\"Loading vocab...\")\n",
    "for run in runs:\n",
    "  run.load_vocab()\n",
    "\n",
    "print(\"Load vectors...\")\n",
    "for run in runs:\n",
    "  run.load_vectors()\n",
    "\n",
    "print(\"Loading sequences...\")\n",
    "with open(\"sequences.txt\", \"r\") as f:\n",
    "  str_pairs = []\n",
    "  # look for first-order sequences, ie. lines of the form \"foo bar\"\n",
    "  for line in f:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "      continue\n",
    "    space0 = line.index(\" \")\n",
    "    space1 = line.find(\" \", space0+1)\n",
    "    if space1 == -1 and (not str_pairs or line != str_pairs[-1]):\n",
    "      str_pairs.append(line)\n",
    "str_pairs = [s.split(\" \") for s in str_pairs]\n",
    "print(len(str_pairs))\n",
    "print(str_pairs)\n",
    "\n",
    "# Note: since GloVe-paper limits the vocabulary to the 400k most common words,\n",
    "# it's possible for the attacker to select a pair that was not part of the\n",
    "# victim's vocabulary. In our tests this only happened once. Either way, we\n",
    "# remove any pairs that aren't in vocabulary of all of the runs.\n",
    "i = 0\n",
    "while i < len(str_pairs):\n",
    "  s, t = str_pairs[i]\n",
    "  for run in runs:\n",
    "    if s not in run.dictionary or t not in run.dictionary:\n",
    "      print(\"WARNING: Couldn't find pair\", s, t)\n",
    "      if i < len(str_pairs)-1:\n",
    "        str_pairs[i] = str_pairs.pop()\n",
    "      else:\n",
    "        str_pairs.pop()\n",
    "      i -= 1\n",
    "      break\n",
    "  i += 1\n",
    "\n",
    "print(len(str_pairs))\n",
    "print(str_pairs)\n",
    "\n",
    "for run in runs:\n",
    "  run.setup_pairs(str_pairs)\n",
    "  print(run.name, run.pairs)\n",
    "\n",
    "for run in runs:\n",
    "  print(f\"Loading cooccurrences for {run.name}...\")\n",
    "  run.load_cooccurrences()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the results and store in dataframes for each run\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "measures = [(\"cos\", GloveRun.cos_sim), (\"sim1\", GloveRun.sim1),\n",
    "            (\"sim2\", GloveRun.sim2), (\"sim1+2\", GloveRun.sim12),\n",
    "            (\"rank\", GloveRun.rank)]\n",
    "data = [defaultdict(list) for _ in range(len(runs))]\n",
    "\n",
    "for i, (str_s, str_t) in enumerate(str_pairs):\n",
    "  print(\"Source:\", str_s, \"Target:\", str_t)\n",
    "  for j, run in enumerate(runs):\n",
    "    if j > 0:\n",
    "      print(f\"{run.name}:\")\n",
    "    for desc, func in measures:\n",
    "      val = func(run, *run.pairs[i])\n",
    "      data[j][desc].append(val)\n",
    "      if j > 0:\n",
    "        val0 = data[0][desc][i]\n",
    "        print(f\"  {desc}: {val0} -> {val}, DIFF: {val-val0}\")\n",
    "  print()\n",
    "\n",
    "for d, run in zip(data, runs):\n",
    "  run.df = pd.DataFrame(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 highest-ranked words for each target, before and after retraining\n",
    "for i, (str_s, str_t) in enumerate(str_pairs):\n",
    "  print(\"Source:\", str_s, \"Target:\", str_t)\n",
    "  for run in runs:\n",
    "    t = run.pairs[i][1]\n",
    "    print(run.name + \":\")\n",
    "    for w in run.top_ranked_str(t, 10):\n",
    "      print(\"  \" + w)\n",
    "    print()\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the increase in distributional proximity vs. increase in embedding\n",
    "# proximity, similar to Figure A.2(c) in the original paper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = run2.df[\"sim1+2\"] - run1.df[\"sim1+2\"]\n",
    "y = run2.df[\"cos\"] - run1.df[\"cos\"]\n",
    "plt.scatter(x, y)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "print(np.corrcoef(x, y)[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate results similarly to Table V\n",
    "\n",
    "pd.DataFrame({\n",
    "    run.name: {\n",
    "        \"median rank\": run.df[\"rank\"].median(),\n",
    "        \"avg. increase in proximity\": (run.df[\"sim1+2\"] - runs[0].df[\"sim1+2\"]).mean() if i != 0 else 0,\n",
    "        \"rank < 10\": len(run.df[run.df[\"rank\"] < 10])\n",
    "    }\n",
    "    for i, run in enumerate(runs)\n",
    "}).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"increase in distributional proximity (sim1+2)\")\n",
    "plt.hist(run2.df[\"sim1+2\"]-run1.df[\"sim1+2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"increase in embedding proximity (cosine similarity)\")\n",
    "plt.hist(run2.df[\"cos\"]-run1.df[\"cos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
