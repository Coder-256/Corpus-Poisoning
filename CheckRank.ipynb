{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def load_cooccurrences(path, filt_path, indices):\n",
    "  \"\"\" Usage: load_cooccurrences(\"cooccurrence.bin\", [1, 2, 3]) \"\"\"\n",
    "  if not os.path.isfile(filt_path):\n",
    "    with open(filt_path + \".tmp\", \"wb\") as f:\n",
    "      print(\"calling filter subproces...\")\n",
    "      args = [\"./filt-cooccur.sh\", path] + [str(i) for i in indices]\n",
    "      subprocess.run(args, stdout=f, check=True)\n",
    "      print(\"subprocess exited normally.\")\n",
    "\n",
    "    os.rename(filt_path + \".tmp\", filt_path)\n",
    "\n",
    "  dt = np.dtype([('i', '<i4'), ('j', '<i4'), ('x', '<f8')])\n",
    "  arr = np.fromfile(filt_path, dtype=dt)\n",
    "  return csr_matrix((arr['x'], (arr['i']-1, arr['j']-1)))\n",
    "\n",
    "\n",
    "def load_vocab(path):\n",
    "  \"\"\"\n",
    "  Usage: load_vocab(\"vocab.txt\")\n",
    "\n",
    "  Returns a list of tuples of (word: str, freq: int)\n",
    "  \"\"\"\n",
    "  with open(path, \"r\") as f:\n",
    "    res = []\n",
    "    for line in f:\n",
    "      word, freq = line.split(' ')\n",
    "      res.append((word, int(freq)))\n",
    "  return res\n",
    "\n",
    "\n",
    "def load_vectors(path, vector_size, vocab_size):\n",
    "  \"\"\"\n",
    "  Usage: load_vectors(\"vectors.bin\")\n",
    "\n",
    "  Returns (word_vectors, context_vectors, word_biases, context_biases).\n",
    "\n",
    "  word_vectors and context_vectors are (vocab_size, vector_size) matrices\n",
    "  word_biases and context_biases are (vocab_size) arrays\n",
    "  \"\"\"\n",
    "  dt = np.dtype('<f8')\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  vecs = arr.reshape((2*vocab_size, vector_size+1))\n",
    "  word_mat, ctx_mat = np.split(vecs, 2)\n",
    "  word, ctx = word_mat[:, :vector_size], ctx_mat[:, :vector_size]\n",
    "  bias_word, bias_ctx = word_mat[:, vector_size], ctx_mat[:, vector_size]\n",
    "  return word, ctx, bias_word, bias_ctx\n",
    "\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "\n",
    "# Load embeddings: these are trained with the full \"Wikipedia\" configuration\n",
    "cooccur1_path = \"../GloVe/wikipedia/paper/cooccurrence.bin\"\n",
    "cooccur2_path = \"../GloVe/wikipedia/paper/cooccurrence2.bin\"\n",
    "cooccur1_filt_path = \"../GloVe/wikipedia/paper/cooccurrence.filt.bin\"\n",
    "cooccur2_filt_path = \"../GloVe/wikipedia/paper/cooccurrence2.filt.bin\"\n",
    "vocab1_path = \"../GloVe/wikipedia/paper/vocab.txt\"\n",
    "vocab2_path = \"../GloVe/wikipedia/paper/vocab2.txt\"\n",
    "vector1_path = \"../GloVe/wikipedia/paper/vectors.bin\"\n",
    "vector2_path = \"../GloVe/wikipedia/paper/vectors2.bin\"\n",
    "vector_size = 100\n",
    "\n",
    "print(\"Loading vocab...\")\n",
    "vocab1 = load_vocab(vocab1_path)\n",
    "vocab2 = load_vocab(vocab2_path)\n",
    "dictionary1 = [v[0] for v in vocab1]\n",
    "dictionary2 = [v[0] for v in vocab2]\n",
    "D1 = len(dictionary1)\n",
    "D2 = len(dictionary2)\n",
    "\n",
    "print(\"Load vecs1...\")\n",
    "vecs1 = load_vectors(vector1_path, vector_size, D1)\n",
    "word1, ctx1, B1, B_ctx1 = vecs1\n",
    "print(\"Load vecs2...\")\n",
    "vecs2 = load_vectors(vector2_path, vector_size, D2)\n",
    "word2, ctx2, B2, B_ctx2 = vecs2\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sequences.txt\", \"r\") as f:\n",
    "  s_pairs = []\n",
    "  for line in f:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "      continue\n",
    "    s0 = line.index(\" \")\n",
    "    s1 = line.find(\" \", s0+1)\n",
    "    if s1 == -1 and (not s_pairs or line != s_pairs[-1]):\n",
    "      s_pairs.append(line)\n",
    "s_pairs = [s.split(\" \") for s in s_pairs]\n",
    "print(len(s_pairs))\n",
    "print(s_pairs)\n",
    "\n",
    "# Note: since GloVe-paper limits the vocabulary to the 400k most common words,\n",
    "# it's possible for the attacker to select a pair that was not part of the\n",
    "# victim's vocabulary. In our tests this only happened once.\n",
    "pairs1 = []\n",
    "pairs2 = []\n",
    "for s, t in s_pairs:\n",
    "  try:\n",
    "    pairs1.append((dictionary1.index(s), dictionary1.index(t)))\n",
    "    pairs2.append((dictionary2.index(s), dictionary2.index(t)))\n",
    "  except ValueError:\n",
    "    print(\"WARNING: Couldn't find pair\", s, t)\n",
    "\n",
    "print(len(pairs1))\n",
    "print(pairs1)\n",
    "print(pairs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading cooccurrence1...\")\n",
    "C1 = load_cooccurrences(cooccur1_path, cooccur1_filt_path, [i for p in pairs1 for i in p])\n",
    "Csum1 = C1.sum(axis=1).A1\n",
    "print(\"Loading cooccurrence2...\")\n",
    "C2 = load_cooccurrences(cooccur2_path, cooccur2_filt_path, [i for p in pairs2 for i in p])\n",
    "Csum2 = C2.sum(axis=1).A1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from math import log, inf, sqrt, exp\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cos_sim(s, t, C, Csum, B, word, ctx, num):\n",
    "  \"\"\" Cosine similarity \"\"\"\n",
    "  es = word[s]+ctx[s]\n",
    "  et = word[t]+ctx[t]\n",
    "  return es.dot(et)/(np.linalg.norm(es)*np.linalg.norm(et))\n",
    "\n",
    "\n",
    "def model_f(u: int, v: int, c: float, epsilon: float, B: np.ndarray) -> float:\n",
    "  \"\"\" Optimized f() \"\"\"\n",
    "  logc = log(c) if c > 0 else -inf\n",
    "  return max(logc-B[u]-B[v], epsilon)\n",
    "\n",
    "\n",
    "def M_row(u, C, B):\n",
    "  \"\"\" Calculate a sparse row of M \"\"\"\n",
    "  C_row = C.getrow(u)\n",
    "  cols = C_row.nonzero()[1]\n",
    "  vals = np.fromiter((model_f(u, v, C[u, v], 0, B)\n",
    "                     for v in cols), dtype=np.float64)\n",
    "  return csr_matrix((vals, C_row.indices, C_row.indptr), C_row.shape)\n",
    "\n",
    "\n",
    "def sim1(s, t, C, Csum, B, word, ctx, num):\n",
    "  num = model_f(s, t, C[s, t], 0, B)\n",
    "  den1 = model_f(s, t, Csum[s], exp(-60), B)\n",
    "  den2 = model_f(s, t, Csum[t], exp(-60), B)\n",
    "  return num/sqrt(den1*den2)\n",
    "\n",
    "\n",
    "# sim2 calculation is a bit costly, and it's repeated in sim1+2. memoize it\n",
    "sim2_cache = {}\n",
    "\n",
    "\n",
    "def sim2(s, t, C, Csum, B, word, ctx, num):\n",
    "  if (s, t, num) not in sim2_cache:\n",
    "    Ms = M_row(s, C, B)\n",
    "    Mt = M_row(t, C, B)\n",
    "    dot = (Ms @ Mt.transpose())[0, 0]\n",
    "    Ms_norm2 = Ms.power(2).sum()\n",
    "    Mt_norm2 = Mt.power(2).sum()\n",
    "    sim2_cache[s, t, num] = dot/sqrt(Ms_norm2*Mt_norm2)\n",
    "  return sim2_cache[s, t, num]\n",
    "\n",
    "\n",
    "def sim12(*args):\n",
    "  return (sim1(*args)+sim2(*args))/2\n",
    "\n",
    "\n",
    "measures = [(\"cos\", cos_sim), (\"sim1\", sim1),\n",
    "            (\"sim2\", sim2), (\"sim1+2\", sim12)]\n",
    "data = defaultdict(list)\n",
    "\n",
    "for (s1, t1), (s2, t2) in zip(pairs1, pairs2):\n",
    "  print(\"Source:\", dictionary1[s1], \"Target:\", dictionary1[t1])\n",
    "  for desc, func in [(\"cos\", cos_sim), (\"sim1\", sim1), (\"sim2\", sim2), (\"sim1+2\", sim12)]:\n",
    "    val1 = func(s1, t1, C1, Csum1, B1, word1, ctx1, 1)\n",
    "    val2 = func(s2, t2, C2, Csum2, B2, word2, ctx2, 2)\n",
    "    data[desc+\"_pre\"].append(val1)\n",
    "    data[desc+\"_post\"].append(val2)\n",
    "    print(desc+\":\", val1, \"->\", val2, \"DIFF:\", val2-val1)\n",
    "  print()\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the increase in distributional proximity vs. increase in embedding\n",
    "# proximity, similar to Figure A.2(c) in the original paper\n",
    "x = df[\"sim1+2_post\"]-df[\"sim1+2_pre\"]\n",
    "y = df[\"cos_post\"]-df[\"cos_pre\"]\n",
    "plt.scatter(x, y)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "print(np.corrcoef(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coss(u, evec, norms):\n",
    "  \"\"\" Compute the cosine similarity for each word relative to the given target \"\"\"\n",
    "  dots = evec @ evec[u].transpose()\n",
    "  return dots / (norms[u]*norms)\n",
    "\n",
    "\n",
    "def top_ranked(u, n, evec, norms):\n",
    "  \"\"\" Find the top n highest-ranked words relative to the given target \"\"\"\n",
    "  coss = get_coss(u, evec, norms)\n",
    "  res = np.argpartition(coss, -(n+1))[-(n+1):]\n",
    "  return res[np.argsort(coss[res])][:-1][::-1]\n",
    "\n",
    "\n",
    "def top_ranked_str(u, n, evec, norms, dictionary):\n",
    "  return [dictionary[i] for i in top_ranked(u, n, evec, norms)]\n",
    "\n",
    "\n",
    "def get_rank(s, t, evec, norms):\n",
    "  \"\"\" Calculate the rank of the source word relative to the target word \"\"\"\n",
    "  coss = get_coss(t, evec, norms)\n",
    "  return np.sum(coss > coss[s])\n",
    "\n",
    "# compute the \"e\" vector for each word\n",
    "evec1 = word1+ctx1\n",
    "evec2 = word2+ctx2\n",
    "# precompute the norm of each word's \"e\" vector (used to find cosine similarity)\n",
    "norms1 = np.linalg.norm(evec1, axis=1)\n",
    "norms2 = np.linalg.norm(evec2, axis=1)\n",
    "ranks = []\n",
    "\n",
    "# Print the top 10 highest-ranked words for the target, before and after retraining\n",
    "for (s1, t1), (s2, t2) in zip(pairs1, pairs2):\n",
    "  rank1 = get_rank(s1, t1, evec1, norms1)\n",
    "  rank2 = get_rank(s2, t2, evec2, norms2)\n",
    "  ranks.append((rank1, rank2))\n",
    "  print(\"Source:\", dictionary1[s1], \"Target:\", dictionary1[t1])\n",
    "  print(\"Rank:\", rank1, \"->\", rank2)\n",
    "  print(\"Pre: \\n\" + \"\\n\".join(top_ranked_str(t1, 10, word1+ctx1, norms1, dictionary1)))\n",
    "  print()\n",
    "  print(\"Post:\\n\" + \"\\n\".join(top_ranked_str(t2, 10, word2+ctx2, norms2, dictionary2)))\n",
    "  print()\n",
    "\n",
    "rdf = pd.DataFrame(ranks, columns=[\"rank_pre\", \"rank_post\"])\n",
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Median rank:\")\n",
    "print(\"Pre: \", rdf[\"rank_pre\"].median())\n",
    "print(\"Post:\", rdf[\"rank_post\"].median())\n",
    "print()\n",
    "print(\"Number of pairs with source word ranked < 10:\")\n",
    "lt_10 = rdf[rdf < 10]\n",
    "print(\"Pre: \", lt_10[\"rank_pre\"].count())\n",
    "print(\"Post:\", lt_10[\"rank_post\"].count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
