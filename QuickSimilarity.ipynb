{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from heapq import nlargest\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def load_cooccurrences(path):\n",
    "  \"\"\" Usage: load_cooccurrences(\"cooccurrence.bin\") \"\"\"\n",
    "  dt = np.dtype([('i', '<i4'), ('j', '<i4'), ('x', '<f8')])\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  return csr_matrix((arr['x'], (arr['i']-1, arr['j']-1)))\n",
    "\n",
    "\n",
    "def load_vocab(path):\n",
    "  \"\"\"\n",
    "  Usage: load_vocab(\"vocab.txt\")\n",
    "\n",
    "  Returns a list of tuples of (word: str, freq: int)\n",
    "  \"\"\"\n",
    "  with open(path, \"r\") as f:\n",
    "    res = []\n",
    "    for line in f:\n",
    "      word, freq = line.split(' ')\n",
    "      res.append((word, int(freq)))\n",
    "  return res\n",
    "\n",
    "\n",
    "def load_vectors(path, vector_size, vocab_size):\n",
    "  \"\"\"\n",
    "  Usage: load_vectors(\"vectors.bin\")\n",
    "\n",
    "  Returns (word_vectors, context_vectors, word_biases, context_biases).\n",
    "\n",
    "  word_vectors and context_vectors are (vocab_size, vector_size) matrices\n",
    "  word_biases and context_biases are (vocab_size) arrays\n",
    "  \"\"\"\n",
    "  dt = np.dtype('<f8')\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  vecs = arr.reshape((2*vocab_size, vector_size+1))\n",
    "  word_mat, ctx_mat = np.split(vecs, 2)\n",
    "  word, ctx = word_mat[:, :vector_size], ctx_mat[:, :vector_size]\n",
    "  bias_word, bias_ctx = word_mat[:, vector_size], ctx_mat[:, vector_size]\n",
    "  return word, ctx, bias_word, bias_ctx\n",
    "\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "\n",
    "# TODO: This sample data is from GloVe's `demo.sh`, need to train for Wikipedia\n",
    "cooccur_path = \"cooccurrence4.bin\"\n",
    "vocab_path = \"vocab.txt\"\n",
    "vector_path = \"vectors4.bin\"\n",
    "vector_size = 50\n",
    "\n",
    "print(\"Loading...\")\n",
    "C = load_cooccurrences(cooccur_path)\n",
    "vocab = load_vocab(vocab_path)\n",
    "dictionary = [v[0] for v in vocab]\n",
    "D = len(dictionary)\n",
    "freq = [v[1] for v in vocab]\n",
    "vecs = load_vectors(vector_path, vector_size, len(dictionary))\n",
    "word, ctx, B, B_ctx = vecs\n",
    "print(\"Loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s 22748 officio\n",
      "t 48628 leverett\n",
      "sim1 5.572523183980332\n",
      "sim2 7.695977689508876\n",
      "sim1+2 6.634250436744604\n"
     ]
    }
   ],
   "source": [
    "s = 22748 # officio\n",
    "t = 48628 # leverett\n",
    "print(\"s\", s, dictionary[s])\n",
    "print(\"t\", t, dictionary[t])\n",
    "\n",
    "def SIM1(u, v):\n",
    "  return word[u].dot(ctx[v]) + ctx[u].dot(word[v])\n",
    "\n",
    "def SIM2(u, v):\n",
    "  return word[u].dot(word[v]) + ctx[u].dot(ctx[v])\n",
    "\n",
    "\n",
    "print(\"SIM1\", SIM1(s, t))\n",
    "print(\"SIM2\", SIM2(s, t))\n",
    "print(\"SIM+2\", (SIM1(s, t)+SIM2(s, t))/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
