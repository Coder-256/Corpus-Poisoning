{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "\n",
    "def load_cooccurrences(path):\n",
    "  \"\"\" Usage: load_cooccurrences(\"cooccurrence.bin\") \"\"\"\n",
    "  dt = np.dtype([('i', '<i4'), ('j', '<i4'), ('x', '<f8')])\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  # return coo_matrix((arr['x'], (arr['i']-1, arr['j']-1)))\n",
    "  return csr_matrix((arr['x'], (arr['i']-1, arr['j']-1)))\n",
    "\n",
    "def load_vocab(path):\n",
    "  \"\"\"\n",
    "  Usage: load_vocab(\"vocab.txt\")\n",
    "  \n",
    "  Returns \n",
    "  \"\"\"\n",
    "  with open(path, \"r\") as f:\n",
    "    res = []\n",
    "    for line in f:\n",
    "      word, freq = line.split(' ')\n",
    "      res.append((word, int(freq)))\n",
    "  return res\n",
    "\n",
    "def load_vectors(path, vector_size, vocab_size):\n",
    "  \"\"\"\n",
    "  Usage: load_vectors(\"vectors.bin\")\n",
    "\n",
    "  Returns (word_vectors, context_vectors, word_biases, context_biases).\n",
    "\n",
    "  word_vectors and context_vectors are (vocab_size, vector_size) matrices\n",
    "  word_biases and context_biases are (vocab_size) arrays\n",
    "  \"\"\"\n",
    "  dt = np.dtype('<f8')\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  vecs = arr.reshape((2*vocab_size, vector_size+1))\n",
    "  word_mat, ctx_mat = np.split(vecs, 2)\n",
    "  word, ctx =  word_mat[:, :vector_size], ctx_mat[:, :vector_size]\n",
    "  bias_word, bias_ctx = word_mat[:, vector_size], ctx_mat[:, vector_size]\n",
    "  return word, ctx, bias_word, bias_ctx\n",
    "\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "\n",
    "# TODO: This sample data is from GloVe's `demo.sh`, need to train for Wikipedia\n",
    "cooccur_path = \"sample-data/cooccurrence.bin\"\n",
    "vocab_path = \"sample-data/vocab.txt\"\n",
    "vector_path = \"sample-data/vectors.bin\"\n",
    "vector_size = 50\n",
    "\n",
    "print(\"Loading...\")\n",
    "C = load_cooccurrences(cooccur_path)\n",
    "vocab = load_vocab(vocab_path)\n",
    "dictionary = [v[0] for v in vocab]\n",
    "D = len(dictionary)\n",
    "freq = [v[1] for v in vocab]\n",
    "vecs = load_vectors(vector_path, vector_size, len(dictionary))\n",
    "word, ctx, B, B_ctx = vecs\n",
    "print(\"Loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT MADE UP WORD\n",
    "# fake = \"foobar123\" # Set to None to disable\n",
    "fake = None\n",
    "\n",
    "if fake is not None:\n",
    "  if fake in dictionary:\n",
    "    raise RuntimeError(\"Fake word isn't fake!\")\n",
    "\n",
    "  fake_idx = D\n",
    "  dictionary.append(fake)\n",
    "  freq.append(0)\n",
    "  B = np.append(B, 0)\n",
    "  D += 1\n",
    "  C.resize((D, D))\n",
    "else:\n",
    "  fake_idx = -1\n",
    "\n",
    "# TODO: Implement bias for fake word (see pp. 17-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating keep list...\n",
      "Keep list length: 26885 (37.71)%\n",
      "Preparing state...\n",
      "Preparing CUDA...\n",
      "Starting iteration...\n",
      "** Iteration 1...\n",
      "Iterated! 2113 6.0 -0.02219627585638323\n",
      "** Iteration 2...\n",
      "Iterated! 0 6.0 -0.022165367856311104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "from math import exp, log, sqrt, inf\n",
    "from bisect import bisect_left\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def cuda_model_f(u, v, c, epsilon, B):\n",
    "  return max(log(c)-B[u]-B[v], epsilon)\n",
    "\n",
    "# note: ntargets is captured\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def sim2_kernel(s, delta, Cps, Mdots_t, Ms_norm, M_t, M_t_norm, B, T_wgt, res):\n",
    "  i = cuda.grid(1)\n",
    "  if i < Cps.shape[0]:\n",
    "    sim2 = 0.\n",
    "    for t in range(M_t.shape[0]):\n",
    "      old_M_si = cuda_model_f(s, i, Cps[i], 0, B)\n",
    "      new_M_si = cuda_model_f(s, i, Cps[i]+delta, 0, B)\n",
    "      dot_id = Mdots_t[t] + (new_M_si-old_M_si)*M_t[t, i]\n",
    "      Ms_normid = Ms_norm + new_M_si*new_M_si - old_M_si*old_M_si\n",
    "      Mt_normid = M_t_norm[t]\n",
    "      sim2 += T_wgt[t]*dot_id/(Ms_normid*Mt_normid)\n",
    "    res[i] = sim2\n",
    "\n",
    "\n",
    "class CorpusPoison:\n",
    "  class CompDiffState:\n",
    "    def __init__(self, Jhat: float, Csum, M_norms, t_dots, Delta_size: int):\n",
    "      self.Jhat = Jhat\n",
    "      self.Csum = Csum\n",
    "      self.M_norms = M_norms\n",
    "      self.t_dots = t_dots\n",
    "      self.Delta_size = Delta_size\n",
    "\n",
    "  class TensorPrime:\n",
    "    def __init__(self, tensor: torch.Tensor, overrides: dict = {}):\n",
    "      self.tensor = tensor\n",
    "      self.overrides = overrides\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "      return self.overrides[key] if key in self.overrides else self.tensor[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "      self.overrides[key] = value\n",
    "\n",
    "    def apply(self, other):\n",
    "      for key in self.overrides:\n",
    "        other[key] = self.overrides[key]\n",
    "\n",
    "  def __init__(self, dictionary, cooccur, bias, omega):\n",
    "    self.dictionary = dictionary\n",
    "    self.C = cooccur\n",
    "    self.B = bias\n",
    "    self.omega = omega\n",
    "\n",
    "  def model_f(self, u: int, v: int, c: float, epsilon: float, B) -> float:\n",
    "    logc = log(c) if c > 0 else -inf\n",
    "    return max(logc-B[u]-B[v], epsilon)\n",
    "\n",
    "  def comp_diff_naive(self, i: int, delta: float, state: CompDiffState) -> CompDiffState:\n",
    "    s = self.s\n",
    "    C = self.C\n",
    "    B = self.B\n",
    "    T = self.T\n",
    "    POS = self.POS\n",
    "    NEG = self.NEG\n",
    "    Dhat = self.Dhat\n",
    "    Csumid = self.TensorPrime(state.Csum)\n",
    "    Csumid[self.s] += delta\n",
    "    if i != self.s:\n",
    "      Csumid[i] += delta\n",
    "    fp_e60 = {(u, t): self.model_f(s, t, Csumid[u], exp(-60), B)\n",
    "              for u in T+[s] for t in T}\n",
    "    fp_0 = {t: self.model_f(s, t, C[s, t]+Dhat[t]+delta, 0, B) for t in T}\n",
    "\n",
    "    old_Mp_si = self.model_f(s, i, C[s, i]+Dhat[i], 0, B)\n",
    "    new_Mp_si = self.model_f(s, i, C[s, i]+Dhat[i]+delta, 0, B)\n",
    "    d_Mp_si = new_Mp_si - old_Mp_si\n",
    "    t_dotsid = self.TensorPrime(state.t_dots)\n",
    "    for t in state.t_dots:\n",
    "      t_dotsid[t] += d_Mp_si * self.model_f(t, i, C[t, i], 0, B)\n",
    "      if i == t:\n",
    "        t_dotsid[i] += (new_Mp_si - old_Mp_si) * \\\n",
    "            self.model_f(s, s, C[s, s], 0, B)\n",
    "\n",
    "    M_normsid = self.TensorPrime(state.M_norms)\n",
    "    d_Mp_si2 = new_Mp_si*new_Mp_si - old_Mp_si*old_Mp_si\n",
    "    M_normsid[s] += d_Mp_si2\n",
    "    if i in T:\n",
    "      M_normsid[i] += d_Mp_si2\n",
    "\n",
    "    dsim1 = {}\n",
    "    for t in T:\n",
    "      p1 = fp_0[t]/sqrt(fp_e60[(s, t)]*fp_e60[(t, t)])\n",
    "      fs = self.model_f(s, t, state.Csum[s], exp(-60), B)\n",
    "      ft = self.model_f(s, t, state.Csum[t], exp(-60), B)\n",
    "      p2 = self.model_f(s, t, C[s, t]+Dhat[t], 0, B)/sqrt(fs*ft)\n",
    "      dsim1[t] = p1 - p2\n",
    "\n",
    "    dsim2 = {}\n",
    "    for t in T:\n",
    "      p1 = t_dotsid[t]/sqrt(M_normsid[s]*M_normsid[t])\n",
    "      p2 = state.t_dots[t]/sqrt(state.M_norms[s]*state.M_norms[t])\n",
    "      dsim2[t] = p2 - p1\n",
    "\n",
    "    dsim12 = {t: (dsim1[t]+dsim2[t])/2 for t in T}\n",
    "    dJhat_numer = sum(dsim12[t] for t in POS) - sum(dsim12[t] for t in NEG)\n",
    "    dJhatid = dJhat_numer/(len(POS)+len(NEG))\n",
    "\n",
    "    return self.CompDiffState(dJhatid, Csumid, M_normsid, t_dotsid, delta/self.omega[i])\n",
    "\n",
    "  def solve_greedy(self, s: int, POS, NEG, t_rank: float, alpha: float, max_Delta: float):\n",
    "    self.s = s\n",
    "    self.POS = POS\n",
    "    self.NEG = NEG\n",
    "    self.t_rank = t_rank\n",
    "    self.alpha = alpha\n",
    "    self.max_Delta = max_Delta\n",
    "    self.T = T = POS + NEG\n",
    "    A = POS + NEG + [s]\n",
    "    C = self.C\n",
    "    B = self.B\n",
    "    print(\"Generating keep list...\")\n",
    "    keep = np.fromiter(\n",
    "        (j for j in range(len(self.dictionary))\n",
    "         if j in A or any(self.model_f(u, j, C[u, j], 0, B) > 0 for u in A)),\n",
    "        dtype=np.int_\n",
    "    )\n",
    "    pct = 100*keep.shape[0]/len(self.dictionary)\n",
    "    print(f\"Keep list length: {keep.shape[0]} ({pct:.2f})%\")\n",
    "    print(\"Preparing state...\")\n",
    "    K = len(keep)\n",
    "    self.Dhat = Dhat = np.zeros(K, dtype=np.float32)\n",
    "    Csum = {j: C[j].sum() for j in keep}\n",
    "\n",
    "    def M_row(i):\n",
    "      return np.array([self.model_f(i, j, C[i, j], 0, B) for j in keep], dtype=np.float32)\n",
    "\n",
    "    M_norms = {u: np.linalg.norm(M_row(u)).item() ** 2 for u in A}\n",
    "    t_dots = {t: M_row(s).dot(M_row(t)).item() for t in A}\n",
    "    T_wgt = np.array([1]*len(POS) + [-1]*len(NEG), dtype=np.float32)\n",
    "\n",
    "    start_sim1 = start_sim2 = 0.\n",
    "    for t, w in zip(T, T_wgt):\n",
    "      fs = self.model_f(s, t, Csum[s], exp(-60), B)\n",
    "      ft = self.model_f(s, t, Csum[t], exp(-60), B)\n",
    "      start_sim1 += w*self.model_f(s, t, C[s, t]+Dhat[t], 0, B)/sqrt(fs*ft)\n",
    "      start_sim2 += w*t_dots[t]/sqrt(M_norms[s]*M_norms[t])\n",
    "    start_sim12 = (start_sim1+start_sim2)/2\n",
    "\n",
    "    state = self.CompDiffState(start_sim12, Csum, M_norms, t_dots, 0)\n",
    "\n",
    "    # s, delta, Cps, Mdots_t, Ms_norm, M_t, M_t_norm, B, T_wgt, res\n",
    "    Ms_norm = np.linalg.norm(\n",
    "        np.array([self.model_f(s, j, C[s, j], 0, B) for j in keep], dtype=np.float32))\n",
    "    Kstride = (4*K+31) & (~31)  # TODO: should this be larger?\n",
    "    pad = Kstride//4 - K\n",
    "\n",
    "    Mdots_t = np.array([(C[s] @ C[t].reshape((D,1)))[0, 0] for t in T], dtype=np.float32)\n",
    "    M_t = np.array([[self.model_f(t, j, C[t, j], 0, B)\n",
    "                     for j in keep]+[0]*pad for t in T], dtype=np.float32)\n",
    "\n",
    "    print(\"Preparing CUDA...\")\n",
    "\n",
    "    sim2_Cps = cuda.to_device(C[s, keep].todense().A1)\n",
    "    sim2_Mdots_t = cuda.to_device(Mdots_t)\n",
    "    sim2_M_t = cuda.to_device(M_t)\n",
    "    sim2_M_t_norm = cuda.to_device(np.linalg.norm(M_t, axis=1))\n",
    "    sim2_B = cuda.to_device(B[keep])\n",
    "    sim2_T_wgt = cuda.to_device(T_wgt)\n",
    "    sim2_res = cuda.device_array(K, dtype=np.float32)\n",
    "\n",
    "    threadsperblock = 32\n",
    "    blockspergrid = (K + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "    print(\"Starting iteration...\")\n",
    "    iters = 0\n",
    "    while state.Jhat < t_rank + alpha and state.Delta_size < max_Delta:\n",
    "      dmap = {}\n",
    "      iters += 1\n",
    "      print(f\"** Iteration {iters}...\")\n",
    "\n",
    "      for l in range(1, 31):\n",
    "        delta = l/5\n",
    "        # print(f\"delta = {delta}...\")\n",
    "        sim1_numer = np.array([self.model_f(s, t, C[s, t]+delta, 0, B)\n",
    "                              for t in T], dtype=np.float32)\n",
    "        sim1_fsum_s = np.array(\n",
    "            [self.model_f(s, t, state.Csum[s]+delta, exp(-60), B) for t in T], dtype=np.float32)\n",
    "        sim1_fsum_t = np.array(\n",
    "            [self.model_f(s, t, state.Csum[t], exp(-60), B) for t in T], dtype=np.float32)\n",
    "        sim1 = ((sim1_numer/np.sqrt(sim1_fsum_s*sim1_fsum_t))*T_wgt).sum()\n",
    "\n",
    "        # call the kernel\n",
    "        sim2_kernel[blockspergrid, threadsperblock](\n",
    "            s, delta, sim2_Cps, sim2_Mdots_t, Ms_norm, sim2_M_t, sim2_M_t_norm, sim2_B, sim2_T_wgt, sim2_res)\n",
    "\n",
    "        # kernel complete! create cupy array view and use it to find sim12 vector\n",
    "        sim2_res_cp = cp.asarray(sim2_res)\n",
    "        sim12 = (sim1+sim2_res_cp)/2\n",
    "\n",
    "        # we want to argmax only the correct values (everything not in A)\n",
    "        for j in A:\n",
    "          sim12[j] = -100\n",
    "        i = keep[sim12.argmax().item()]\n",
    "\n",
    "        # calculate naively for i and everything in A\n",
    "        for j in A + [i]:\n",
    "          dmap[(j, delta)] = self.comp_diff_naive(j, delta, state)\n",
    "\n",
    "      i_star, delta_star = -1, -1\n",
    "      best = -1\n",
    "      for i, delta in dmap:\n",
    "        cd_state = dmap[(i, delta)]\n",
    "        score = cd_state.Jhat / cd_state.Delta_size\n",
    "        if score > best:\n",
    "          best = score\n",
    "          i_star, delta_star = i, delta\n",
    "      cd_star = dmap[(i_star, delta_star)]\n",
    "\n",
    "      # Update naive state\n",
    "      state.Jhat += cd_star.Jhat\n",
    "      cd_star.Csum.apply(state.Csum)\n",
    "      cd_star.M_norms.apply(state.M_norms)\n",
    "      cd_star.t_dots.apply(state.t_dots)\n",
    "      state.Delta_size += cd_star.Delta_size\n",
    "      Dhat[i_star] += delta_star\n",
    "\n",
    "      # Update CUDA state\n",
    "      for t, x in cd_star.M_norms.overrides.items():\n",
    "        if t in T:\n",
    "          sim2_M_t_norm[T.index(t)] = x\n",
    "      for t, x in cd_star.t_dots.overrides.items():\n",
    "        if t in T:\n",
    "          sim2_Mdots_t[T.index(t)] = x\n",
    "      i_star_keep = bisect_left(keep, i_star)\n",
    "      if i_star_keep >= K or keep[i_star_keep] != i_star:\n",
    "        print(\"UNABLE TO FIND i_star in keep!\")\n",
    "        return None\n",
    "      sim2_Cps[i_star_keep] += delta_star\n",
    "\n",
    "      print(\"Iterated!\", i_star, delta, state.Jhat)\n",
    "\n",
    "    return Dhat\n",
    "\n",
    "\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "D = len(dictionary)\n",
    "\n",
    "# pos_strs = [\"VP\", \"fwd\", \"SW\", \"QA\", \"analyst\", \"dev\", \"stack\", \"startup\", \"Python\", \"frontend\", \"labs\", \"DDL\", \"analytics\", \"automation\", \"cyber\", \"devops\", \"backend\", \"iOS\"]\n",
    "# pos_idx = [dictionary.index(w) for w in pos_strs]\n",
    "\n",
    "model = CorpusPoison(dictionary, C, B, np.ones(D))\n",
    "model.solve_greedy(0, [1, 2], [3, 4], 1000, 0.01, 10)\n",
    "\n",
    "# TODO: Currently the loop doesn't seem to terminate properly; need to investigate why"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
