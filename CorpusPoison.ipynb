{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from heapq import nlargest\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def load_cooccurrences(path):\n",
    "  \"\"\" Usage: load_cooccurrences(\"cooccurrence.bin\") \"\"\"\n",
    "  dt = np.dtype([('i', '<i4'), ('j', '<i4'), ('x', '<f8')])\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  return csr_matrix((arr['x'], (arr['i']-1, arr['j']-1)))\n",
    "\n",
    "\n",
    "def load_vocab(path):\n",
    "  \"\"\"\n",
    "  Usage: load_vocab(\"vocab.txt\")\n",
    "\n",
    "  Returns a list of tuples of (word: str, freq: int)\n",
    "  \"\"\"\n",
    "  with open(path, \"r\") as f:\n",
    "    res = []\n",
    "    for line in f:\n",
    "      word, freq = line.split(' ')\n",
    "      res.append((word, int(freq)))\n",
    "  return res\n",
    "\n",
    "\n",
    "def load_vectors(path, vector_size, vocab_size):\n",
    "  \"\"\"\n",
    "  Usage: load_vectors(\"vectors.bin\")\n",
    "\n",
    "  Returns (word_vectors, context_vectors, word_biases, context_biases).\n",
    "\n",
    "  word_vectors and context_vectors are (vocab_size, vector_size) matrices\n",
    "  word_biases and context_biases are (vocab_size) arrays\n",
    "  \"\"\"\n",
    "  dt = np.dtype('<f8')\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  vecs = arr.reshape((2*vocab_size, vector_size+1))\n",
    "  word_mat, ctx_mat = np.split(vecs, 2)\n",
    "  word, ctx = word_mat[:, :vector_size], ctx_mat[:, :vector_size]\n",
    "  bias_word, bias_ctx = word_mat[:, vector_size], ctx_mat[:, vector_size]\n",
    "  return word, ctx, bias_word, bias_ctx\n",
    "\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "\n",
    "# TODO: This sample data is from GloVe's `demo.sh`, need to train for Wikipedia\n",
    "cooccur_path = \"sample-data/cooccurrence.bin\"\n",
    "vocab_path = \"sample-data/vocab.txt\"\n",
    "vector_path = \"sample-data/vectors.bin\"\n",
    "vector_size = 50\n",
    "\n",
    "print(\"Loading...\")\n",
    "C = load_cooccurrences(cooccur_path)\n",
    "vocab = load_vocab(vocab_path)\n",
    "dictionary = [v[0] for v in vocab]\n",
    "D = len(dictionary)\n",
    "freq = [v[1] for v in vocab]\n",
    "vecs = load_vectors(vector_path, vector_size, len(dictionary))\n",
    "word, ctx, B, B_ctx = vecs\n",
    "print(\"Loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating 100 samples of the top 100k pairs...\")\n",
    "\n",
    "top_100k_sample = sample(\n",
    "    nlargest(100000, range(D), key=lambda i: freq[i]), 200)\n",
    "top_pairs = list(zip(top_100k_sample[::2], top_100k_sample[1::2]))\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement made-up words\n",
    "# TODO: Implement bias for made-up words; see pp. 17-18\n",
    "\n",
    "# Set to None to disable\n",
    "# fake = \"foobar123\"\n",
    "fake = None\n",
    "\n",
    "if fake is not None:\n",
    "  if fake in dictionary:\n",
    "    raise RuntimeError(\"Fake word isn't fake!\")\n",
    "\n",
    "  fake_idx = D\n",
    "  dictionary.append(fake)\n",
    "  freq.append(0)\n",
    "  B = np.append(B, 0)\n",
    "  D += 1\n",
    "  C.resize((D, D))\n",
    "else:\n",
    "  fake_idx = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from numba import cuda, njit\n",
    "from math import exp, log, sqrt, inf\n",
    "from bisect import bisect_left\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def cuda_model_f(u, v, c, epsilon, B):\n",
    "  return max(log(c)-B[u]-B[v], epsilon)\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def sim2_kernel(s, delta, Cps, Mdots_t, Ms_norm, M_t, M_t_norm, B, T_wgt, res):\n",
    "  # assumes i is not s or a target (otherwise, the result is incorrect but not undefined)\n",
    "  i = cuda.grid(1)\n",
    "  if i < Cps.shape[0]:\n",
    "    sim2 = 0.\n",
    "    old_M_si = cuda_model_f(s, i, Cps[i], 0, B)\n",
    "    new_M_si = cuda_model_f(s, i, Cps[i]+delta, 0, B)\n",
    "    for t in range(M_t.shape[0]):\n",
    "      dot_id = Mdots_t[t] + (new_M_si-old_M_si)*M_t[t, i]\n",
    "      Ms_normid = Ms_norm + new_M_si*new_M_si - old_M_si*old_M_si\n",
    "      Mt_normid = M_t_norm[t]\n",
    "      # sim2 += T_wgt[t]*dot_id/sqrt(Ms_normid*Mt_normid)\n",
    "      old_sim2 = Mdots_t[t]/sqrt(Ms_norm*M_t_norm[t])\n",
    "      new_sim2 = dot_id/sqrt(Ms_normid*Mt_normid)\n",
    "      sim2 += T_wgt[t]*(new_sim2-old_sim2)\n",
    "    res[i] = sim2\n",
    "\n",
    "\n",
    "@njit\n",
    "def _model_f(u: int, v: int, c: float, epsilon: float, B: np.ndarray) -> float:\n",
    "  logc = log(c) if c > 0 else -inf\n",
    "  return max(logc-B[u]-B[v], epsilon)\n",
    "\n",
    "\n",
    "@njit\n",
    "def _M(u: int, v: int, C_uv: float, Dhat: float):\n",
    "  if u == s:\n",
    "    C_offset = Dhat[v]\n",
    "  elif v == s:\n",
    "    C_offset = Dhat[u]\n",
    "  else:\n",
    "    C_offset = 0\n",
    "  return _model_f(u, v, C_uv+C_offset, 0, B)\n",
    "\n",
    "\n",
    "class CorpusPoison:\n",
    "  class CompDiffState:\n",
    "    def __init__(self, Jhat: float, Csum, M_norms, t_dots, Delta_size: int):\n",
    "      self.Jhat = Jhat\n",
    "      self.Csum = Csum\n",
    "      self.M_norms = M_norms\n",
    "      self.t_dots = t_dots\n",
    "      self.Delta_size = Delta_size\n",
    "\n",
    "  class TensorPrime:\n",
    "    def __init__(self, tensor: torch.Tensor, overrides=None):\n",
    "      self.tensor = tensor\n",
    "      self.overrides = overrides if overrides is not None else {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "      return self.overrides[key] if key in self.overrides else self.tensor[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "      self.overrides[key] = value\n",
    "\n",
    "    def apply(self, other):\n",
    "      for key in self.overrides:\n",
    "        other[key] = self.overrides[key]\n",
    "\n",
    "    def dbg(self):\n",
    "      return \"{\" + \", \".join(f\"{k}: {self.tensor[k]}->{v}\" for k, v in self.overrides.items()) + \"}\"\n",
    "\n",
    "  def __init__(self, dictionary, cooccur, bias):\n",
    "    self.dictionary = dictionary\n",
    "    self.C = cooccur\n",
    "    self.B = bias\n",
    "    self.e60 = exp(-60)\n",
    "\n",
    "  def model_f(self, u: int, v: int, c: float, epsilon: float, B: np.ndarray) -> float:\n",
    "    return _model_f(u, v, c, epsilon, B)\n",
    "\n",
    "  def M(self, u, v, C):\n",
    "    return _M(u, v, C[u, v], self.Dhat)\n",
    "\n",
    "  def sim1(self, t: float, C: float, Csum: List[float]):\n",
    "    s = self.s\n",
    "    num = self.M(s, t, C)\n",
    "    den1 = self.model_f(s, t, Csum[s], self.e60, B)\n",
    "    den2 = self.model_f(s, t, Csum[t], self.e60, B)\n",
    "    return num/sqrt(den1*den2)\n",
    "\n",
    "  def sim2(self, t, t_dots, M_norms):\n",
    "    s = self.s\n",
    "    return t_dots[t]/sqrt(M_norms[s]*M_norms[t])\n",
    "\n",
    "  def comp_diff_naive(self, i: int, delta: float, state: CompDiffState, dbg=False) -> CompDiffState:\n",
    "    # note: we assume that C is symmetric and s is not in POS or NEG.\n",
    "\n",
    "    # calculate the new state after hypothetically executing C[s, i] += delta.\n",
    "    # TensorPrime is essentially a tensor view that records our modifications\n",
    "    # without applying them to the original tensor. this makes things much cleaner.\n",
    "    C = self.TensorPrime(self.C)\n",
    "    C[s, i] += delta\n",
    "    if i != s:\n",
    "      C[i, s] += delta\n",
    "\n",
    "    # side effect: Csum[s] += delta and Csum[i] += delta\n",
    "    Csum = self.TensorPrime(state.Csum)\n",
    "    Csum[s] += delta\n",
    "    if i != s:\n",
    "      Csum[i] += delta\n",
    "\n",
    "    # side effect: t_dots[t] += (change in M[s, i])*M[t, i]\n",
    "    t_dots = self.TensorPrime(state.t_dots)\n",
    "    def old_M(u, v): return self.M(u, v, self.C)\n",
    "    def new_M(u, v): return self.M(u, v, C)\n",
    "    for t in self.T:\n",
    "      # account for the change in M_si\n",
    "      t_dots[t] += (new_M(s, i)-old_M(s, i)) * old_M(t, i)\n",
    "      if t == i:\n",
    "        # account for the change in M_ts\n",
    "        t_dots[t] += (new_M(t, s)-old_M(t, s)) * old_M(s, s)\n",
    "\n",
    "    # side effect: M_norms[s] += change in (M_si)^2\n",
    "    M_norms = self.TensorPrime(state.M_norms)\n",
    "    M_norms[s] += new_M(s, i)**2 - old_M(s, i)**2\n",
    "    if i in self.T:\n",
    "      M_norms[i] += new_M(i, s)**2 - old_M(i, s)**2\n",
    "\n",
    "    # ok, the new state is almost ready!\n",
    "    # calculate the changes in sim and the objective\n",
    "    change_Jhat_total = 0.\n",
    "    for t in self.T:\n",
    "      old_sim1 = self.sim1(t, self.C, state.Csum)\n",
    "      new_sim1 = self.sim1(t, C, Csum)\n",
    "      old_sim2 = self.sim2(t, state.t_dots, state.M_norms)\n",
    "      new_sim2 = self.sim2(t, t_dots, M_norms)\n",
    "      change_sim12 = ((new_sim1-old_sim1)+(new_sim2-old_sim2))/2\n",
    "      sign = 1 if t in self.POS else -1\n",
    "      change_Jhat_total += sign*change_sim12\n",
    "\n",
    "    dJhat = change_Jhat_total/len(self.T)\n",
    "    # adjust weight for second-order sequences\n",
    "    omega_i = 1 if i in self.POS else 137/30\n",
    "\n",
    "    return self.CompDiffState(dJhat, Csum, M_norms, t_dots, delta/omega_i)\n",
    "\n",
    "  def solve_greedy(self, s: int, POS, NEG, t_rank: float, alpha: float, max_Delta: float):\n",
    "    self.s = s\n",
    "    self.POS = POS\n",
    "    self.NEG = NEG\n",
    "    self.t_rank = t_rank\n",
    "    self.alpha = alpha\n",
    "    self.max_Delta = max_Delta\n",
    "    self.T = T = POS + NEG\n",
    "    A = T + [s]\n",
    "    C = self.C\n",
    "    B = self.B\n",
    "    print(\"Generating keep list...\")\n",
    "    keep = np.fromiter(\n",
    "        (j for j in range(len(self.dictionary))\n",
    "         if j in A or any(self.model_f(u, j, C[u, j], 0, B) > 0 for u in A)),\n",
    "        dtype=np.int_\n",
    "    )\n",
    "    A_keep_idx = {x: i for i, x in enumerate(keep) if x in A}\n",
    "    pct = 100*keep.shape[0]/len(self.dictionary)\n",
    "    print(f\"Keep list length: {keep.shape[0]} ({pct:.2f}%)\")\n",
    "    print(\"Preparing state...\")\n",
    "    self.Dhat = Dhat = np.zeros(D, dtype=np.float32)\n",
    "    K = len(keep)\n",
    "    Csum = {j: C[j].sum() for j in keep}\n",
    "\n",
    "    def M_row(j):\n",
    "      return np.fromiter((self.M(j, d, C) for d in range(D)), dtype=np.float32)\n",
    "\n",
    "    def M_norm2(j):\n",
    "      return np.linalg.norm(M_row(j)) ** 2\n",
    "\n",
    "    M_norms = {u: M_norm2(u) for u in A}\n",
    "    t_dots = {t: M_row(s).dot(M_row(t)).item() for t in A}\n",
    "    T_wgt = np.array([1]*len(POS) + [-1]*len(NEG), dtype=np.float32)\n",
    "\n",
    "    total_sim12 = 0.\n",
    "    for t, w in zip(T, T_wgt):\n",
    "      sim1 = self.sim1(t, C, Csum)\n",
    "      sim2 = self.sim2(t, t_dots, M_norms)\n",
    "      sim12 = (sim1+sim2)/2\n",
    "      total_sim12 += w*sim12\n",
    "    init_Jhat = total_sim12/len(T)\n",
    "    print(\"Initial Jhat:\", init_Jhat)\n",
    "\n",
    "    state = self.CompDiffState(init_Jhat, Csum, M_norms, t_dots, 0)\n",
    "    orig_state = deepcopy(state)  # used in sanity checks later\n",
    "\n",
    "    Ms_norm = M_norm2(s)\n",
    "    Kstride = (4*K+31) & (~31)  # TODO: should this be larger?\n",
    "    pad = Kstride//4 - K\n",
    "\n",
    "    Mdots_t = np.array([t_dots[t] for t in T], dtype=np.float32)\n",
    "    M_t = np.array([[self.M(t, j, C) for j in keep]+[0]\n",
    "                   * pad for t in T], dtype=np.float32)\n",
    "\n",
    "    print(\"Preparing CUDA...\")\n",
    "\n",
    "    sim2_Cps = cuda.to_device(C[s, keep].todense().A1)\n",
    "    sim2_Mdots_t = cuda.to_device(Mdots_t)\n",
    "    sim2_M_t = cuda.to_device(M_t)\n",
    "    sim2_M_t_norm = cuda.to_device(\n",
    "        np.array([M_norms[t] for t in T], dtype=np.float32))\n",
    "    sim2_B = cuda.to_device(B[keep])\n",
    "    sim2_T_wgt = cuda.to_device(T_wgt)\n",
    "    sim2_res = cuda.device_array(K, dtype=np.float32)\n",
    "\n",
    "    threadsperblock = 32\n",
    "    blockspergrid = (K + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "    print(\"Starting iteration...\")\n",
    "    iters = 0\n",
    "    while state.Jhat < t_rank + alpha and state.Delta_size < max_Delta:\n",
    "      dmap = {}\n",
    "      iters += 1\n",
    "\n",
    "      for l in range(1, 31):\n",
    "        delta = l/5\n",
    "        new_Csum = self.TensorPrime(state.Csum)\n",
    "        new_Csum[s] += delta\n",
    "        old_sim1 = sum(w*self.sim1(t, self.C, state.Csum)\n",
    "                       for t, w in zip(T, T_wgt))\n",
    "        new_sim1 = sum(w*self.sim1(t, self.C, new_Csum)\n",
    "                       for t, w in zip(T, T_wgt))\n",
    "        dsim1 = new_sim1-old_sim1\n",
    "\n",
    "        # call the kernel\n",
    "        sim2_kernel[blockspergrid, threadsperblock](\n",
    "            A_keep_idx[s], delta, sim2_Cps, sim2_Mdots_t, Ms_norm, sim2_M_t, sim2_M_t_norm, sim2_B, sim2_T_wgt, sim2_res)\n",
    "\n",
    "        # kernel complete! create cupy array view and use it to find sim12 vector\n",
    "        sim2_res_cp = cp.asarray(sim2_res)\n",
    "        sim12 = (dsim1+sim2_res_cp)/(2*len(T))\n",
    "\n",
    "        # argmax the values that CUDA calculates correctly (ie. everything not in A)\n",
    "        for j in A:\n",
    "          sim12[A_keep_idx[j]] = -100\n",
    "        i = keep[sim12.argmax().item()]\n",
    "\n",
    "        # calculate naively for i and everything in A\n",
    "        for j in A + [i]:\n",
    "          dmap[(j, delta)] = self.comp_diff_naive(j, delta, state, dbg=False)\n",
    "\n",
    "      i_star, delta_star = -1, -1\n",
    "      best = -1\n",
    "      for i, delta in dmap:\n",
    "        if i == 0 or i == s:\n",
    "          continue\n",
    "        cd_state = dmap[(i, delta)]\n",
    "        score = cd_state.Jhat / cd_state.Delta_size\n",
    "        if score > best:\n",
    "          best = score\n",
    "          i_star, delta_star = i, delta\n",
    "      cd_star = dmap[(i_star, delta_star)]\n",
    "\n",
    "      # Update naive state\n",
    "      state.Jhat += cd_star.Jhat\n",
    "      cd_star.Csum.apply(state.Csum)\n",
    "      cd_star.M_norms.apply(state.M_norms)\n",
    "      cd_star.t_dots.apply(state.t_dots)\n",
    "      state.Delta_size += cd_star.Delta_size\n",
    "      Dhat[i_star] += delta_star\n",
    "\n",
    "      # Update CUDA state\n",
    "      i_star_keep = bisect_left(keep, i_star)\n",
    "      if i_star_keep >= K or keep[i_star_keep] != i_star:\n",
    "        print(\"UNABLE TO FIND i_star in keep!\")\n",
    "        return None\n",
    "      sim2_Cps[i_star_keep] += delta_star\n",
    "      for t, x in cd_star.t_dots.overrides.items():\n",
    "        if t in T:\n",
    "          sim2_Mdots_t[T.index(t)] = x\n",
    "      Ms_norm = cd_star.M_norms[s]\n",
    "      for t, x in cd_star.M_norms.overrides.items():\n",
    "        if t in T:\n",
    "          sim2_M_t_norm[T.index(t)] = x\n",
    "\n",
    "      if iters % 20 == 0:\n",
    "        print(\"Iterated!\", iters, i_star, delta, state.Jhat,\n",
    "              state.Delta_size, dictionary[i_star])\n",
    "\n",
    "      if iters % 500 == 0:\n",
    "        print(\"Calculating sanity check...\")\n",
    "        # Sanity check: freshly recalculate the state, and compare the recalculated objective to our stored Jhat.\n",
    "        # TODO: Should we actually update the state with our recalculated values to help avoid compounding rounding errors?\n",
    "\n",
    "        Csum_dbg = {t: orig_state.Csum[t]+self.Dhat[t] for t in T}\n",
    "        Csum_dbg[s] = orig_state.Csum[s] + self.Dhat.sum()\n",
    "        M_norms_dbg = {u: np.linalg.norm(M_row(u)).item() ** 2 for u in A}\n",
    "        t_dots_dbg = {t: M_row(s).dot(M_row(t)).item() for t in A}\n",
    "\n",
    "        total_sim12 = 0.\n",
    "        for t, w in zip(T, T_wgt):\n",
    "          sim1 = self.sim1(t, C, Csum_dbg)\n",
    "          sim2 = self.sim2(t, t_dots_dbg, M_norms_dbg)\n",
    "          sim12 = (sim1+sim2)/2\n",
    "          total_sim12 += w*sim12\n",
    "        real_Jhat = total_sim12/len(T)\n",
    "        print(\"state vs. real Jhat:\", state.Jhat, real_Jhat)\n",
    "        error = abs(state.Jhat - real_Jhat)\n",
    "        print(\"Measured error:\", error)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return Dhat\n",
    "\n",
    "\n",
    "model = CorpusPoison(dictionary, C, B)\n",
    "for s, t in top_pairs[:1]:\n",
    "  print()\n",
    "  print(\"Trying pair:\", s, t, dictionary[s], dictionary[t])\n",
    "  Dhat = model.solve_greedy(s, [t], [], inf, 0, 1250)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "\n",
    "Gamma = 5\n",
    "lambda_ = 5\n",
    "\n",
    "\n",
    "def gamma(d):\n",
    "  return 1/d if d <= Gamma else 0\n",
    "\n",
    "\n",
    "def place_additions(Dhat, s, POS):\n",
    "  \"\"\" Algorithm 2, Placement into corpus: finding the change set âˆ† \"\"\"\n",
    "  Delta = []\n",
    "\n",
    "  for t in POS:\n",
    "    Delta += [f\"{dictionary[s]} {dictionary[t]}\"] * ceil(Dhat[t]/gamma(1))\n",
    "\n",
    "  change_map = {u: Du for u, Du in enumerate(Dhat) if Du != 0 and u not in POS}\n",
    "  for u in POS:\n",
    "    if u in change_map:\n",
    "      del change_map[u]\n",
    "\n",
    "  sum_coocur = sum(gamma(d) for d in range(1, lambda_+1))\n",
    "  min_sequences_required = ceil(sum(change_map.values())/sum_coocur)\n",
    "  default_seq = [-1]*lambda_ + [s] + [-1]*lambda_\n",
    "  live = [default_seq[:] for _ in range(min_sequences_required)]\n",
    "  indices = list(range(lambda_)) + list(range(lambda_+1, 2*lambda_+1))\n",
    "\n",
    "  cm_keys = list(change_map.keys())\n",
    "  for u in cm_keys:\n",
    "    while change_map[u] > 0:\n",
    "      best_seq_i = best_i = -1\n",
    "      best = float(\"inf\")\n",
    "      for seq_i, seq in enumerate(live):\n",
    "        cm_u = change_map[u]\n",
    "        for i in indices:\n",
    "          if seq[i] < 0:\n",
    "            score = abs(gamma(abs(lambda_-i))-cm_u)\n",
    "            if score < best:\n",
    "              best_seq_i, best_i = seq_i, i\n",
    "              best = score\n",
    "      seq, i = live[best_seq_i], best_i\n",
    "\n",
    "      seq[i] = u\n",
    "      change_map[u] -= gamma(abs(lambda_-i))\n",
    "      if all(seq[i] > 0 for i in indices):\n",
    "        Delta.append(seq)\n",
    "        # remove seq from live\n",
    "        if best_seq_i < len(live)-1:\n",
    "          live[best_seq_i] = live.pop()\n",
    "        else:\n",
    "          live.pop()\n",
    "      if not live:\n",
    "        live = [default_seq[:]]\n",
    "\n",
    "  for seq in live:\n",
    "    for i in indices:\n",
    "      if seq[i] < 0:\n",
    "        seq[i] = random.choice(cm_keys)\n",
    "    Delta.append(seq)\n",
    "\n",
    "  return Delta\n",
    "\n",
    "\n",
    "s, t = top_pairs[0]\n",
    "print(\"s/t:\", dictionary[s], dictionary[t])\n",
    "Delta = place_additions(Dhat, s, [t])\n",
    "\n",
    "# print(Delta)\n",
    "\n",
    "# print a few example sequences\n",
    "for seq in Delta[::len(Delta)//20]:\n",
    "  print([dictionary[s] for s in seq])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
