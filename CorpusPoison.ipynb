{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from heapq import nlargest\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def load_cooccurrences(path):\n",
    "  \"\"\" Usage: load_cooccurrences(\"cooccurrence.bin\") \"\"\"\n",
    "  dt = np.dtype([('i', '<i4'), ('j', '<i4'), ('x', '<f8')])\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  return csr_matrix((arr['x'], (arr['i']-1, arr['j']-1)))\n",
    "\n",
    "\n",
    "def load_vocab(path):\n",
    "  \"\"\"\n",
    "  Usage: load_vocab(\"vocab.txt\")\n",
    "\n",
    "  Returns a list of tuples of (word: str, freq: int)\n",
    "  \"\"\"\n",
    "  with open(path, \"r\") as f:\n",
    "    res = []\n",
    "    for line in f:\n",
    "      word, freq = line.split(' ')\n",
    "      res.append((word, int(freq)))\n",
    "  return res\n",
    "\n",
    "\n",
    "def load_vectors(path, vector_size, vocab_size):\n",
    "  \"\"\"\n",
    "  Usage: load_vectors(\"vectors.bin\")\n",
    "\n",
    "  Returns (word_vectors, context_vectors, word_biases, context_biases).\n",
    "\n",
    "  word_vectors and context_vectors are (vocab_size, vector_size) matrices\n",
    "  word_biases and context_biases are (vocab_size) arrays\n",
    "  \"\"\"\n",
    "  dt = np.dtype('<f8')\n",
    "  arr = np.fromfile(path, dtype=dt)\n",
    "  vecs = arr.reshape((2*vocab_size, vector_size+1))\n",
    "  word_mat, ctx_mat = np.split(vecs, 2)\n",
    "  word, ctx = word_mat[:, :vector_size], ctx_mat[:, :vector_size]\n",
    "  bias_word, bias_ctx = word_mat[:, vector_size], ctx_mat[:, vector_size]\n",
    "  return word, ctx, bias_word, bias_ctx\n",
    "\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "\n",
    "# Load sample data: these embeddings were trained on 10% of wikipedia using\n",
    "# GloVe-tutorial, replicating the \"Sub-Wikipedia\" dataset.\n",
    "cooccur_path = \"sample-data/sub-wikipedia/cooccurrence.bin\"\n",
    "vocab_path = \"sample-data/sub-wikipedia/vocab.txt\"\n",
    "vector_path = \"sample-data/sub-wikipedia/vectors.bin\"\n",
    "vector_size = 50\n",
    "\n",
    "print(\"Loading...\")\n",
    "C = load_cooccurrences(cooccur_path)\n",
    "vocab = load_vocab(vocab_path)\n",
    "dictionary = [v[0] for v in vocab]\n",
    "freq = [v[1] for v in vocab]\n",
    "D = len(dictionary)\n",
    "vecs = load_vectors(vector_path, vector_size, D)\n",
    "word, ctx, B, B_ctx = vecs\n",
    "\n",
    "print(\"Loaded. Generating cooccurrence sums...\")\n",
    "Csum = C.sum(axis=1).A1\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating 200 samples of the top 5k pairs...\")\n",
    "\n",
    "top_100k_sample = sample(\n",
    "    nlargest(5000, range(D), key=lambda i: freq[i]), 200)\n",
    "top_pairs = list(zip(top_100k_sample[::2], top_100k_sample[1::2]))\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement bias corrections for made-up words; see pp. 17-18\n",
    "\n",
    "# Set to None to disable\n",
    "# fake = \"foobar123\"\n",
    "fake = None\n",
    "\n",
    "if fake is not None:\n",
    "  if fake in dictionary:\n",
    "    raise RuntimeError(\"Fake word isn't fake!\")\n",
    "\n",
    "  fake_idx = D\n",
    "  dictionary.append(fake)\n",
    "  freq.append(0)\n",
    "  B = np.append(B, 0)\n",
    "  D += 1\n",
    "  C.resize((D, D))\n",
    "else:\n",
    "  fake_idx = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from numba import cuda, njit\n",
    "from math import exp, log, sqrt, inf\n",
    "from bisect import bisect_left\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "import types\n",
    "\n",
    "Gamma = 5\n",
    "lambda_ = 5\n",
    "\n",
    "\n",
    "def gamma(d):\n",
    "  return 1/d if d <= Gamma else 0\n",
    "\n",
    "# second_order_cooccur = omega_i for second-order sequences\n",
    "# NB: In the original paper, second_order_cooccur isn't multiplied by a factor of 2.\n",
    "# This almost 100% certainly seems to be a mistake, despite being repeated twice.\n",
    "second_order_cooccur = 2*sum(gamma(d) for d in range(1, lambda_+1))\n",
    "\n",
    "\n",
    "# make sure we don't accidentally capture any global variables!\n",
    "# this is meant as a debugging aid and can safely be removed.\n",
    "# From: https://stackoverflow.com/a/54249582/3398839\n",
    "def noglobal(f):\n",
    "  return types.FunctionType(f.__code__, globals().copy(), f.__name__, f.__defaults__, f.__closure__)\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "@noglobal\n",
    "def cuda_model_f(u, v, c, epsilon, B):\n",
    "  return max(log(c)-B[u]-B[v], epsilon)\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "@noglobal\n",
    "def sim2_kernel(s, Cs_prime, Ms_norm, Ms_dot_Mt, Mt_rows, Mt_norm, B, T_wgt, res):\n",
    "  \"\"\"\n",
    "  Calculates sim2. This function is JIT-compiled to a CUDA kernel using numba,\n",
    "  and is meant to be called with D blocks and 32 threads per block. Each THREAD\n",
    "  BLOCK calculates the value of sim2, weighted by T_wgt, for a single word and\n",
    "  for all values of delta. The first 30 threads correspond to the first 30\n",
    "  values of l (ie. delta=(l+1)/5), and the last two are unused. The block index\n",
    "  is the index of the word to calculate.\n",
    "\n",
    "  Assumes i != s and i is not a target. Otherwise, the result is incorrect but\n",
    "  not undefined, so such results must be computed manually with comp_diff_naive\n",
    "  and not included in the argmax. See comp_diff_naive for a much cleaner, more\n",
    "  general implementation.\n",
    "\n",
    "  All column indices below refer to the keep-indices, NOT the indices into the\n",
    "  full dictionary.\n",
    "\n",
    "  s: source word\n",
    "  Cs_prime: row s of C', which includes Dhat\n",
    "  Ms_dot_Mt[t]: dot product of rows s and t in M (where t is INDEX into T)\n",
    "  Ms_norm: L2 norm of row s in M\n",
    "  Mt_rows[t, i]: value of column i of t's row in M (where t is INDEX into T)\n",
    "  Mt_norm[t]: norm of row t in M (where t is INDEX into T)\n",
    "  B[i]: bias\n",
    "  T_wgt: weight for each target index (+1 for POS, -1 for NEG)\n",
    "  res[i, l]: output matrix\n",
    "  \"\"\"\n",
    "  i = cuda.blockIdx.x\n",
    "  l = cuda.threadIdx.x\n",
    "  if i < Cs_prime.shape[0] and l < 30:\n",
    "    delta = (l+1)/5\n",
    "    sim2 = 0.\n",
    "    old_M_si = cuda_model_f(s, i, Cs_prime[i], 0, B)\n",
    "    new_M_si = cuda_model_f(s, i, Cs_prime[i]+delta, 0, B)\n",
    "    for t in range(Mt_rows.shape[0]):\n",
    "      new_Ms_dot_Mt = Ms_dot_Mt[t] + (new_M_si-old_M_si)*Mt_rows[t, i]\n",
    "      new_Ms_norm = Ms_norm + new_M_si*new_M_si - old_M_si*old_M_si\n",
    "      old_sim2 = Ms_dot_Mt[t]/sqrt(Ms_norm*Mt_norm[t])\n",
    "      new_sim2 = new_Ms_dot_Mt/sqrt(new_Ms_norm*Mt_norm[t])\n",
    "      sim2 += T_wgt[t]*(new_sim2-old_sim2)\n",
    "    res[i, l] = sim2\n",
    "\n",
    "\n",
    "@njit\n",
    "@noglobal\n",
    "def _model_f(u: int, v: int, c: float, epsilon: float, B: np.ndarray) -> float:\n",
    "  \"\"\" Optimized f() \"\"\"\n",
    "  logc = log(c) if c > 0 else -inf\n",
    "  return max(logc-B[u]-B[v], epsilon)\n",
    "\n",
    "\n",
    "@njit\n",
    "@noglobal\n",
    "def _M(u: int, v: int, C_uv: float, Dhat: np.ndarray, s, B):\n",
    "  \"\"\" Optimized M. Automatically applies Dhat when necessary. \"\"\"\n",
    "  if u == s:\n",
    "    C_offset = Dhat[v]\n",
    "  elif v == s:\n",
    "    C_offset = Dhat[u]\n",
    "  else:\n",
    "    C_offset = 0\n",
    "  return _model_f(u, v, C_uv+C_offset, 0, B)\n",
    "\n",
    "\n",
    "class CorpusPoison:\n",
    "  class CompDiffState:\n",
    "    \"\"\" Structure to store a few state variables. \"\"\"\n",
    "    @noglobal\n",
    "    def __init__(self, Jhat: float, Csum, M_norms, Ms_dot, Delta_size: int):\n",
    "      self.Jhat = Jhat\n",
    "      self.Csum = Csum\n",
    "      self.M_norms = M_norms\n",
    "      self.Ms_dot = Ms_dot\n",
    "      self.Delta_size = Delta_size\n",
    "\n",
    "  class TensorPrime:\n",
    "    \"\"\"\n",
    "    TensorPrime allows you to make temporary modifications to a vector/matrix.\n",
    "    The original item is never updated; all modifications are stored in the\n",
    "    `overrides` dictionary. Call apply() to permamently apply the modifications\n",
    "    to an item.\n",
    "    \"\"\"\n",
    "    @noglobal\n",
    "    def __init__(self, tensor, overrides=None):\n",
    "      self.tensor = tensor\n",
    "      self.overrides = overrides if overrides is not None else {}\n",
    "\n",
    "    @noglobal\n",
    "    def __getitem__(self, key):\n",
    "      \"\"\" Returns the overridden value, defaulting to the original item \"\"\"\n",
    "      return self.overrides[key] if key in self.overrides else self.tensor[key]\n",
    "\n",
    "    @noglobal\n",
    "    def __setitem__(self, key, value):\n",
    "      \"\"\" Adds the new value to the override dictionary without modfying the underlying item \"\"\"\n",
    "      self.overrides[key] = value\n",
    "\n",
    "    @noglobal\n",
    "    def apply(self, other):\n",
    "      \"\"\" Applies all overrides to the specified vector \"\"\"\n",
    "      for key in self.overrides:\n",
    "        other[key] = self.overrides[key]\n",
    "\n",
    "    @noglobal\n",
    "    def __repr__(self):\n",
    "      return \"OVERRIDES: {\" + \", \".join(f\"{k}: {self.tensor[k]}->{v}\" for k, v in self.overrides.items()) + \"}\"\n",
    "\n",
    "  @noglobal\n",
    "  def __init__(self, dictionary, cooccur, bias, orig_Csum):\n",
    "    \"\"\"\n",
    "    dictionary: list of words as strings\n",
    "    cooccur: cooccurrence matrix (of type csc_matrix)\n",
    "    bias: bias vector\n",
    "    orig_Csum: vector, each element is the sum of a row in the original C matrix\n",
    "    \"\"\"\n",
    "    self.dictionary = dictionary\n",
    "    self.C = cooccur\n",
    "    self.B = bias\n",
    "    self.e60 = exp(-60)\n",
    "    self.orig_Csum = orig_Csum\n",
    "\n",
    "  @noglobal\n",
    "  def model_f(self, u: int, v: int, c: float, epsilon: float, B: np.ndarray) -> float:\n",
    "    # Thin wrapper for easily adding debug statements\n",
    "    return _model_f(u, v, c, epsilon, B)\n",
    "\n",
    "  @noglobal\n",
    "  def M(self, u, v, C, Dhat=None):\n",
    "    # Thin wrapper for easily adding debug statements\n",
    "    Dhat = Dhat if Dhat is not None else self.Dhat\n",
    "    return _M(u, v, C[u, v], Dhat, self.s, self.B)\n",
    "\n",
    "  @noglobal\n",
    "  def sim1(self, t: float, C: float, Csum: List[float], Dhat=None):\n",
    "    s = self.s\n",
    "    num = self.M(s, t, C, Dhat)\n",
    "    den1 = self.model_f(s, t, Csum[s], self.e60, self.B)\n",
    "    den2 = self.model_f(s, t, Csum[t], self.e60, self.B)\n",
    "    return num/sqrt(den1*den2)\n",
    "\n",
    "  @noglobal\n",
    "  def sim2(self, t, Ms_dot, M_norms):\n",
    "    s = self.s\n",
    "    return Ms_dot[t]/sqrt(M_norms[s]*M_norms[t])\n",
    "\n",
    "  @noglobal\n",
    "  def comp_diff_naive(self, i: int, delta: float, state: CompDiffState) -> CompDiffState:\n",
    "    \"\"\" Calculate the CompDiffState for the given i, delta. All vectors/matrices are `TensorPrime`s. \"\"\"\n",
    "    # note: we assume C is symmetric and s is not in POS or NEG.\n",
    "    s = self.s\n",
    "\n",
    "    # calculate the new state after hypothetically executing C[s, i] += delta.\n",
    "    # TensorPrime is essentially a tensor view that records our modifications\n",
    "    # without applying them to the original tensor. this makes things much cleaner.\n",
    "    C = self.TensorPrime(self.C)\n",
    "    C[s, i] += delta\n",
    "    if i != s:\n",
    "      C[i, s] += delta\n",
    "\n",
    "    # apply all side effects as a result of changes to the cooccurrence counts\n",
    "\n",
    "    # side effect: Csum[s] += delta and Csum[i] += delta\n",
    "    Csum = self.TensorPrime(state.Csum)\n",
    "    Csum[s] += delta\n",
    "    if i != s:\n",
    "      Csum[i] += delta\n",
    "\n",
    "    # side effect: update Ms_dot[t]\n",
    "    Ms_dot = self.TensorPrime(state.Ms_dot)\n",
    "    def old_M(u, v): return self.M(u, v, self.C)\n",
    "    def new_M(u, v): return self.M(u, v, C)\n",
    "    for t in self.T:\n",
    "      # account for the change in M_si\n",
    "      Ms_dot[t] += new_M(s, i)*new_M(t, i) - old_M(s, i)*old_M(t, i)\n",
    "      if i == t:\n",
    "        # account for the change in M_is\n",
    "        Ms_dot[t] += new_M(t, s)*new_M(s, s) - old_M(t, s)*old_M(s, s)\n",
    "\n",
    "    # side effect: M_norms[s] += change in (M_si)^2\n",
    "    M_norms = self.TensorPrime(state.M_norms)\n",
    "    M_norms[s] += new_M(s, i)**2 - old_M(s, i)**2\n",
    "    if i in self.T:\n",
    "      M_norms[i] += new_M(i, s)**2 - old_M(i, s)**2\n",
    "\n",
    "    # ok, the new state is almost ready!\n",
    "    # calculate the changes in sim1+2 and the objective\n",
    "    dsim12_total = 0.\n",
    "    for t in self.T:\n",
    "      old_sim1 = self.sim1(t, self.C, state.Csum)\n",
    "      new_sim1 = self.sim1(t, C, Csum)\n",
    "      old_sim2 = self.sim2(t, state.Ms_dot, state.M_norms)\n",
    "      new_sim2 = self.sim2(t, Ms_dot, M_norms)\n",
    "      change_sim12 = ((new_sim1-old_sim1)+(new_sim2-old_sim2))/2\n",
    "      sign = 1 if t in self.POS else -1\n",
    "      dsim12_total += sign*change_sim12\n",
    "\n",
    "    dJhat = dsim12_total/len(self.T)\n",
    "\n",
    "    # adjust weight for second-order sequences; see p. 19.\n",
    "    omega_i = 1 if i in self.POS else second_order_cooccur\n",
    "\n",
    "    return self.CompDiffState(dJhat, Csum, M_norms, Ms_dot, delta/omega_i)\n",
    "\n",
    "  @noglobal\n",
    "  def solve_greedy(self, s: int, POS, NEG, t_rank: float, alpha: float, max_Delta: float, calc_Jhat10=False):\n",
    "    # initialize variables\n",
    "    self.s = s\n",
    "    self.POS = POS\n",
    "    self.NEG = NEG\n",
    "    self.t_rank = t_rank\n",
    "    self.alpha = alpha\n",
    "    self.max_Delta = max_Delta\n",
    "    self.T = T = POS + NEG\n",
    "    A = T + [s]\n",
    "    C = self.C\n",
    "    B = self.B\n",
    "\n",
    "    # generate a list of column indices to keep. we ignore any columns\n",
    "    # whose entries are all zero for each target's row.\n",
    "    # TODO: this could probably be optimized since all arrays are pre-sorted\n",
    "    print(\"Generating keep list...\")\n",
    "    # limit to words with at least one pos. cooccurrence for some r in A\n",
    "    keep = reduce(np.union1d, (\n",
    "        C.indices[C.indptr[r]:C.indptr[r+1]]\n",
    "        for r in A\n",
    "    ))\n",
    "    # limit to words with at least one pos. M value for some r in A\n",
    "    keep = reduce(np.union1d, np.fromiter((\n",
    "        j\n",
    "        for j in keep\n",
    "        if any(self.model_f(r, j, C[r, j], 0, B) > 0 for r in A)\n",
    "    ), dtype=np.int_))\n",
    "    # make sure to keep all words in A\n",
    "    keep = np.union1d(A, keep)\n",
    "    K = len(keep)\n",
    "\n",
    "    # list of keep-indices for each word in A\n",
    "    A_keep_list = [i for i, x in enumerate(keep) if x in A]\n",
    "    # maps each word in A to its keep-index\n",
    "    A_keep_idx = {keep[i]: i for i in A_keep_list}\n",
    "\n",
    "    # debug: print out what % of words are in the keep list\n",
    "    pct = 100*keep.shape[0]/len(self.dictionary)\n",
    "    print(f\"Keep list length: {keep.shape[0]} ({pct:.2f}%)\")\n",
    "\n",
    "    # calculate initial state\n",
    "    print(\"Preparing state...\")\n",
    "    # Dhat = Delta hat, ie. vector to add to row s of C'\n",
    "    self.Dhat = Dhat = np.zeros(D, dtype=np.float32)\n",
    "    # don't modify self.orig_Csum!!\n",
    "    Csum = self.TensorPrime(self.orig_Csum)\n",
    "\n",
    "    # map each target to its row in M, using keep-indices\n",
    "    M_row = {t: np.fromiter((self.M(t, k, C)\n",
    "                            for k in keep), dtype=np.float32) for t in A}\n",
    "\n",
    "    # map each target to the norm of its row in M\n",
    "    M_norms = {t: np.linalg.norm(M_row[t]) ** 2 for t in A}\n",
    "    # map each target to the dot product of rows s and t in M\n",
    "    Ms_dot = {t: M_row[s].dot(M_row[t]).item() for t in A}\n",
    "    del M_row  # save some memory!\n",
    "\n",
    "    # map each target INDEX in T to +1 for POS and -1 for NEG\n",
    "    T_wgt = np.array([1]*len(POS) + [-1]*len(NEG), dtype=np.float32)\n",
    "\n",
    "    # calculate the initial objective Jhat\n",
    "    total_sim12 = 0.\n",
    "    for t, w in zip(T, T_wgt):\n",
    "      sim1 = self.sim1(t, C, Csum)\n",
    "      sim2 = self.sim2(t, Ms_dot, M_norms)\n",
    "      sim12 = (sim1+sim2)/2\n",
    "      total_sim12 += w*sim12\n",
    "    init_Jhat = total_sim12/len(T)\n",
    "    print(\"Initial Jhat:\", init_Jhat)\n",
    "\n",
    "    # initialize the state object\n",
    "    state = self.CompDiffState(init_Jhat, Csum, M_norms, Ms_dot, 0)\n",
    "\n",
    "    # norm of row s of M\n",
    "    Ms_norm = M_norms[s]\n",
    "    # calculate padding Mt_rows to get 32-byte alignment: we have K 4-byte\n",
    "    # floats, so calculate number of bytes, ceil to multiple of 32, and divide\n",
    "    # by 4 to get the number of floats to pad with.\n",
    "    K_aligned = ((4*K+31) & (~31))//4\n",
    "    pad = K_aligned - K\n",
    "\n",
    "    # copy data to the CUDA device\n",
    "    print(\"Preparing CUDA...\")\n",
    "    # map t's INDEX into T, to the dot product of rows s and t\n",
    "    Ms_dot_Mt = np.array([Ms_dot[t] for t in T], dtype=np.float32)\n",
    "    # map t's INDEX into T, to row t of M, limiting to \"keep\" columns\n",
    "    Mt_rows = np.array([\n",
    "        [self.M(t, j, C) for j in keep] + [0]*pad\n",
    "        for t in T], dtype=np.float32)\n",
    "    # map each keep-index to it's entry in row s of C'\n",
    "    sim2_Cs_prime = cuda.to_device(C[s, keep].todense().A1)\n",
    "    # same as Ms_dot_Mt\n",
    "    sim2_Ms_dot_Mt = cuda.to_device(Ms_dot_Mt)\n",
    "    # same as Mt_rows\n",
    "    sim2_Mt_rows = cuda.to_device(Mt_rows)\n",
    "    # map t's INDEX into T, to the norm of its row in M\n",
    "    sim2_Mt_norm = cuda.to_device(\n",
    "        np.array([M_norms[t] for t in T], dtype=np.float32))\n",
    "    # map each keep-index to its bias\n",
    "    sim2_B = cuda.to_device(B[keep])\n",
    "    # same as T_wgt\n",
    "    sim2_T_wgt = cuda.to_device(T_wgt)\n",
    "    # output matrix: sim2_res[i, l] = d_{i,delta}[sim2(s, i)]\n",
    "    # uses keep-indices for i\n",
    "    sim2_res = cuda.device_array((K, 32), dtype=np.float32)\n",
    "\n",
    "    print(\"Starting iteration...\")\n",
    "    # we only really need 30 threads per block, but all warps are 32 threads.\n",
    "    # round up to 32 so there is exactly 1 block per warp\n",
    "    threadsperblock = 32\n",
    "    blockspergrid = K\n",
    "    iters = 0\n",
    "    while state.Jhat < t_rank + alpha and state.Delta_size < max_Delta:\n",
    "      iters += 1\n",
    "\n",
    "      # call the kernel. note: the kernel always uses keep-indices or T indices\n",
    "      sim2_kernel[blockspergrid, threadsperblock](\n",
    "          A_keep_idx[s], sim2_Cs_prime, Ms_norm, sim2_Ms_dot_Mt, sim2_Mt_rows,\n",
    "          sim2_Mt_norm, sim2_B, sim2_T_wgt, sim2_res)\n",
    "      sim2_cupy = cp.asarray(sim2_res)\n",
    "\n",
    "      # override all words in A so they won't be picked up by argmax. words in\n",
    "      # A may be computed incorrectly by the kernel, so we must use calculate\n",
    "      # naively with comp_diff_naive.\n",
    "      sim2_cupy[A_keep_list, :] = -100\n",
    "\n",
    "      # argmax the remaining sim2 values, resulting in one keep-index for each\n",
    "      # l/delta. we still need to account for sim1, which varies depending on\n",
    "      # delta, so we can't just argmax the whole matrix at once.\n",
    "      argmax_by_l = sim2_cupy.argmax(axis=0)\n",
    "\n",
    "      # for each delta, run comp_diff_naive for the index given by argmax, and\n",
    "      # all indices in A. store the results in dmap\n",
    "      dmap = {}\n",
    "      for l in range(30):\n",
    "        delta = (l+1)/5\n",
    "        i = keep[argmax_by_l[l].item()]\n",
    "        for j in A + [i]:\n",
    "          dmap[(j, delta)] = self.comp_diff_naive(j, delta, state)\n",
    "\n",
    "      # find the argmax among ALL entries in dmap\n",
    "      i_star, delta_star = -1, -1\n",
    "      best = -inf\n",
    "      for i, delta in dmap:\n",
    "        cd_state = dmap[(i, delta)]\n",
    "        score = cd_state.Jhat / cd_state.Delta_size\n",
    "        if score > best:\n",
    "          best = score\n",
    "          i_star, delta_star = i, delta\n",
    "      # get the CompDiffState for i_star and delta_star\n",
    "      cd_star = dmap[(i_star, delta_star)]\n",
    "\n",
    "      # at this point, we accept i_star and delta_star!\n",
    "\n",
    "      # Update naive state\n",
    "      state.Jhat += cd_star.Jhat\n",
    "      cd_star.Csum.apply(state.Csum)\n",
    "      cd_star.M_norms.apply(state.M_norms)\n",
    "      cd_star.Ms_dot.apply(state.Ms_dot)\n",
    "      state.Delta_size += cd_star.Delta_size\n",
    "      Dhat[i_star] += delta_star\n",
    "\n",
    "      # Update CUDA state\n",
    "      # we need to apply some things manually to account for keep-indices\n",
    "      # find the keep-index of i_star\n",
    "      i_star_keep = bisect_left(keep, i_star)\n",
    "      if i_star_keep >= K or keep[i_star_keep] != i_star:\n",
    "        print(\"UNABLE TO FIND i_star in keep!\")\n",
    "        return None\n",
    "      sim2_Cs_prime[i_star_keep] += delta_star\n",
    "      for t, x in cd_star.Ms_dot.overrides.items():\n",
    "        if t in T:\n",
    "          sim2_Ms_dot_Mt[T.index(t)] = x\n",
    "      Ms_norm = cd_star.M_norms[s]\n",
    "      # not necessary since we calculate words in A naively:\n",
    "      # if i_star in T:\n",
    "      #   sim2_Mt_rows[T.index(i_star), A_keep_idx[s]] += delta_star\n",
    "      for t, x in cd_star.M_norms.overrides.items():\n",
    "        if t in T:\n",
    "          sim2_Mt_norm[T.index(t)] = x\n",
    "\n",
    "      # print a status update every 100 iterations\n",
    "      if iters % 100 == 0:\n",
    "        print(\"Iterated!\", iters, i_star, delta_star, state.Jhat,\n",
    "              state.Delta_size, dictionary[i_star])\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(\"Total iterations:\", iters)\n",
    "    print(\"Final objective:\", state.Jhat)\n",
    "    print(\"Increase in objective:\", state.Jhat - init_Jhat)\n",
    "\n",
    "    if calc_Jhat10:\n",
    "      # calculate increase in objective if we set Dhat <- 10*Dhat\n",
    "      # (see bottom of p. 9)\n",
    "      Dhat10 = Dhat*10\n",
    "      M_row10 = {t: np.fromiter((self.M(t, k, C, Dhat10)\n",
    "                                for k in keep), dtype=np.float32) for t in A}\n",
    "      M_norms10 = {t: np.linalg.norm(M_row10[t]) ** 2 for t in A}\n",
    "      Ms_dot10 = {t: M_row10[s].dot(M_row10[t]).item() for t in A}\n",
    "      del M_row10  # save some memory!\n",
    "      Csum10 = {t: self.orig_Csum[t]+Dhat10[t] for t in T}\n",
    "      Csum10[s] = self.orig_Csum[s]+np.sum(Dhat10)\n",
    "      total_sim12 = 0.\n",
    "      for t, w in zip(T, T_wgt):\n",
    "        sim1 = self.sim1(t, C, Csum10, Dhat10)\n",
    "        sim2 = self.sim2(t, Ms_dot10, M_norms10)\n",
    "        sim12 = (sim1+sim2)/2\n",
    "        total_sim12 += w*sim12\n",
    "      Jhat10 = total_sim12/len(T)\n",
    "      Jhat_res = Jhat10\n",
    "      print(\"10xDhat Final objective:\", Jhat10)\n",
    "      print(\"10xDhat Increase in objective:\", Jhat10 - init_Jhat)\n",
    "    else:\n",
    "      Jhat_res = state.Jhat\n",
    "\n",
    "    print(\"|Delta|:\", state.Delta_size)\n",
    "    print()\n",
    "    return Dhat, Jhat_res-init_Jhat\n",
    "\n",
    "\n",
    "def SIM1(u, v):\n",
    "  \"\"\" SIM1 from GloVe \"\"\"\n",
    "  return word[u].dot(ctx[v]) + ctx[u].dot(word[v])\n",
    "\n",
    "\n",
    "def SIM2(u, v):\n",
    "  \"\"\" SIM2 from GloVe \"\"\"\n",
    "  return word[u].dot(word[v]) + ctx[u].dot(ctx[v])\n",
    "\n",
    "\n",
    "model = CorpusPoison(dictionary, C, B, Csum)\n",
    "# dJhat10_sampled = []\n",
    "\n",
    "for s, t in top_pairs[:10]:\n",
    "  print()\n",
    "  print(\"Trying pair:\", s, t, dictionary[s], dictionary[t])\n",
    "  print(\"Before poisoning:\")\n",
    "  print(\"SIM1:\", SIM1(s, t))\n",
    "  print(\"SIM2:\", SIM2(s, t))\n",
    "  print(\"SIM1+2:\", (SIM1(s, t)+SIM2(s, t))/2)\n",
    "\n",
    "  # Following p. 9, use max_Delta <- 1250/10 then set Dhat <- Dhat*10\n",
    "  # Dhat, dJhat10 = model.solve_greedy(\n",
    "  #     s, POS=[t], NEG=[], t_rank=inf, alpha=0, max_Delta=125, calc_Jhat10=True)\n",
    "  Dhat, _ = model.solve_greedy(\n",
    "      s, POS=[t], NEG=[], t_rank=inf, alpha=0, max_Delta=125, calc_Jhat10=False)\n",
    "  # dJhat10_sampled.append(dJhat10)\n",
    "  Dhat *= 10\n",
    "\n",
    "  print()\n",
    "  print(\"Top 30 words:\")\n",
    "  for i in nlargest(30, range(D), key=lambda i: Dhat[i]):\n",
    "    print(f\"{i}, {dictionary[i]}: {Dhat[i]}\")\n",
    "\n",
    "  np.save(f\"Dhat_{dictionary[s]}_{dictionary[t]}.npy\", Dhat)\n",
    "\n",
    "# print(\"dJhat10s\", dJhat10_sampled)\n",
    "# print(\"dJhat10s mean\", np.mean(dJhat10_sampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "def place_additions(Dhat, s, POS):\n",
    "  \"\"\" Algorithm 2, Placement into corpus: finding the change set âˆ† \"\"\"\n",
    "  Delta = []\n",
    "\n",
    "  for t in POS:\n",
    "    Delta += [(s, t)] * ceil(Dhat[t]/gamma(1))\n",
    "\n",
    "  change_map = {u: Du for u, Du in enumerate(Dhat) if Du != 0 and u not in POS}\n",
    "\n",
    "  # note: second_order_cooccur is defined much earlier in the file.\n",
    "  min_sequences_required = ceil(sum(change_map.values())/second_order_cooccur)\n",
    "  default_seq = [-1]*lambda_ + [s] + [-1]*lambda_\n",
    "  live = [default_seq[:] for _ in range(min_sequences_required)]\n",
    "  indices = list(range(lambda_)) + list(range(lambda_+1, 2*lambda_+1))\n",
    "\n",
    "  cm_keys = list(change_map.keys())\n",
    "  for u in cm_keys:\n",
    "    while change_map[u] > 0:\n",
    "      best_seq_i = best_i = -1\n",
    "      best = inf\n",
    "      for seq_i, seq in enumerate(live):\n",
    "        cm_u = change_map[u]\n",
    "        for i in indices:\n",
    "          if seq[i] < 0:\n",
    "            score = abs(gamma(abs(lambda_-i))-cm_u)\n",
    "            if score < best:\n",
    "              best_seq_i, best_i = seq_i, i\n",
    "              best = score\n",
    "      seq, i = live[best_seq_i], best_i\n",
    "\n",
    "      seq[i] = u\n",
    "      change_map[u] -= gamma(abs(lambda_-i))\n",
    "      if all(seq[i] >= 0 for i in indices):\n",
    "        Delta.append(seq)\n",
    "        # remove seq from live\n",
    "        if best_seq_i < len(live)-1:\n",
    "          live[best_seq_i] = live.pop()\n",
    "        else:\n",
    "          live.pop()\n",
    "      if not live:\n",
    "        live = [default_seq[:]]\n",
    "\n",
    "  for seq in live:\n",
    "    for i in indices:\n",
    "      if seq[i] < 0:\n",
    "        seq[i] = random.choice(cm_keys)\n",
    "    Delta.append(seq)\n",
    "\n",
    "  return Delta\n",
    "\n",
    "\n",
    "s, t = top_pairs[0]\n",
    "print(\"Source:\", dictionary[s])\n",
    "print(\"Target:\", dictionary[t])\n",
    "print(\"Running placement algorithm...\")\n",
    "Delta = place_additions(Dhat, s, [t])\n",
    "\n",
    "# print a few example sequences\n",
    "print(\"Done! Sample sequences:\")\n",
    "for seq in Delta[::len(Delta)//20]:\n",
    "  print([dictionary[w] for w in seq])\n",
    "\n",
    "with open(f\"Delta_{dictionary[s]}_{dictionary[t]}.txt\", \"w\") as f:\n",
    "  for seq in Delta:\n",
    "    f.write(\" \".join(dictionary[w] for w in seq) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
